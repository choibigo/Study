# Cross Validation 이란?
- 전체 Data를 나누어 형성한 Batch가 모둔 한번씩은 Validation Set으로 활용되도록 각 iteration 마다 validation Set을 바꿔 사용한다.
- 주로 Train 데이터가 작을때 사용한다.

# 회귀 / 분류시 알맞은 Metric은 무엇일까요
- 회귀 : MSE => Ground Truth와 Predict의 값의 차이, 차이 제곱의 평균
- 분류 : Precision, Recall, Accurcy => 분류 결과의 신도를 나타냄

# 알고있는 Metric에 대해 설명
- MSE: 실제 값과 예측값 차이의 제곱
- Precision: 정답이라고 예측한 값들 중 실제 정답 비율
- Recall: 실제 정답 값들중 내가 정답이라고 한 값
- Accuracy : 전체 값들중 실제 정답을 맞춘 비율

# 정규화를 왜 해야하는가?
- 정규화는 개별 피처의 크기를 모두 똑같은 단위로 변경하는 것을 말한다.
- 피처의 스케일이 데이터간 심하게 차이나는 경우 더큰 피쳐가 중요하게 여겨질 수 있기 때문에 이를 막기 위해 사용한다.
- 데이터를 mini-batch로 쪼개거나, 네트워크 깊이가 깊어짐에 따라 각 Feature의 분포 값이 달라져서 학습시 어려움이 생기는 문제가 발생한다.
- Batch Normalization: 각 Batch를 미리 정해둔 평균, 분산값으로 정규화 한다.
- Weight Normalization: 동일한 층의 파라미터 값을 정규화 한다.
- ![image](https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2F9b966d12-f684-4cca-8f7c-0385af93d0ca%2Fimage.png)
- unormalize input feature는 왼쪽처럼 비대칭적이므로 최적의 값을 찾는데 많이 진동한다. 이때는 작은 Learning Rate를 사용해야한다.
- normalize input Feature는 진동하지 않고 최소값을 찾을 수 있어서 큰 Learnig Rate을 사용해도 된다.
- 작은 배치 단위에서는 잘 작동하지 않을 수 있다.
- 학습 시의 분포와 추론시 데이터 분포를 비슷하게 만들 수 있다.
- 가중치에 대해 민감도를 감소 시킨다.

# Local Minima, Global Minima
- Global Minima는 Gradient Descent 방법으로 학습시, 해당 도메인에 가장 낮은 Cost를 가질수 있는 Weight가 위치한 점
- Local Minima는 Global Minima를 찾는 과정에서 지역적으로 Gradient 하강, 상승 구건이 존재하여 빠질 수 있는 가짜 Minima이다.

# 차원의 저주란?(curse of dimensionality)
- 관측치의 수보다 데이터를 표현하는 차원의 개수가 더 많아져 발생하는 현상 입니다.
- 차원이 증가할수록 변수의 수가 증가하고, 개별 차원 내에서 학습할 데이터 수가 적어진다.
- 이 때문에 오히려 데이터를 표현하는 차원이 늘어나도 학습이 어려워지는 문제 이다.

# Deep Learning이 갖는 일반적인 문제점
- 모델이 깊게 설계 할수록 Gradient vanishing 문제가 일어나 학습이 제대로 되지 않는 문제가 존재한다.
- 이를 해결하기 위해 잔차 학습, Batch Normalization 과 같은 기법으로 극복 했습니다.
- 선형 레이어를 깊게 쌓아도 선형적인 데이터를 선형 적으로만 표현 했기때문에 XOR 문제와 같이 복잡한 문제를 풀지 못했다.
- sigmoid나 ReLU와 같은 활성화 함수로 비선형성을 띄게 됬고 복잡한 문제도 풀수 있게 되었다.

# Deep Learning 혁신 근간
- Gradient Descent Optimizer와 Normalization 기법들이 등장하여 모델을 깊게 설계할 수 있게 됬다.
- 또한 깊게 설계해도 GPU연산을 통해 matrix연산을 빨라져 학습을 보다 수월하게 할수 있다.
- 학습을 할수 있는 많은 빅데이터가 존재로 인해 더욱 발달했다.

# ROC 커브란
- TPR: GT가 참인 케이스에 대해 참이라고 예측한 비율 => 커야 좋다.
- FPR: GT가 거짓인 케이스에 참이라고 예측한 비율 => 작아야 좋다.
- 성능이 좋을수록 그래프 좌상단의 꼭지점에 다가가고, 그래프 면적이 넓어야 좋은 그래프가된다.

# 앙상블 기법이란?
- 앙상블은 여러 모델들의 결과를 합쳐서 더 나은 결과를 내는 방법이다.
1) 같은 데이터셋에 대해 다른 모델을 학습시켜 결과를 투표하는 방법인 Voting
2) 같은 모델을 다른 Train Data로 학습 시키는 Bagging

# Feature Vector란
- Feature는 data를 나타내는 측정 가능한 요소이다, Feature Vector는 한 데이터가 가지고 있는 Feature들의 집합 이다.

# 좋은 모델이란?
- Research Level에서는 성능이 높은 모델이 좋은 모델이 될 수 있다.
- 그러나 Production Level에서는 성능은 좀 낮더라도 자원을 덜사용하거나 속도가 더 빠른 모델이 좋은모델이 될 수도 있다.
- 일반적으로는 필요한 리소스만으로 좋은 성능을 내는, 최적화가 잘된 모델을 좋은 모델이라고 할 수 있다.
- 모델이 일반화가 잘 되있어 한번도 보지 못한 데이터도 옳은 판단을 내리는 모델

# 가중치 초기화

## Xavier
- Sigmoid 활성화 함수에서 사용한다.
- 표준 정규분포로 초기화된 가중치를 (1/이전 layer의 노드 수)**0.5 로 나눈다.

## HE
- ReLU 활성화 함수에서 사용한다.
- 표준 정규 분포로 초기화된 가중치를 (2/이전 layer의 노드 수)**0.5로 나눈다.

## 베이지안 최적화란
- 사전 정보를 통해 최적 값 탐색에 반영하는 것