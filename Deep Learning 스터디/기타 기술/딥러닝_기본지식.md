# 인공지능
- 인간이 가지는 인식, 판단 등의 지적 능력을 모델리하여 컴퓨터에서 구현한것

# 머신러닝이란?
- 데이터를 분석하고 해당 데이터를 통해 학습한 후 정보를 바탕으로 결정을 내리기 위해 학습한 내용을 적용하는 알고리즘

# 딥러닝
-  머신 러닝의 한 종류중 하나이다, 뉴럴 네트워크를 통해 학습하는 방법으로 머신러닝의 한 종류이다.

# Linear Regression

## Linear Regression
- 데이터의 분보를 가장 잘 설명할 수 있는 선을 찾는 분석 방법을 Linear Regression이라 한다.

## Hyperthesis란
- Linear Regression에서 사용하는 1차원 방정식을 가리킨다.
- Wx + b 로 W와b에 따라 모델의 결과가 바뀌고 학습 된다.

## Cost
- 학습중에 학습한 결과와 실제 값과 차이를 보기 위한 방법이다.
- Hypothesis와 실제 값과의 차이를 말한다.
- 모든 입력 데이터에 대해 오차를 계산하는 함수를 Cost Function 이라 한다.
- 모든 데이터의 오차, Loss Function의 평균이 된다.

## Loss Function
- 하나의 데이터에 대한 실제 값과 예측값의 차이

## Objective Function
- 모델의 오차를 최대값, 최소값을 구하는 함수이다.
- Object > Loss > Cost

# Sigmoid보다 ReLU를 많이 쓰는데 그이유는?
- Sigmoid는 함수의 특성상 파라미터 값이 0이나 1에 수렴하게 된다.
- 모델의 깊이가 깊어질 수록 파라미터가 특정값으로 수렴하면 기울기가 변화하지 않고 학습이 이루어지지 않는다.
- 그에 반해 ReLU는 출력값이 하나의 값으로 수렴하지 않아서 Backpropagation할 떄, 멀리있느 Layer까지 전달 된다.

## Non-Linearity라는 말의 의미와 필요성은
- 데이터에 맞는 분포를 그릴수 있는 모델을 좋은 모델이라고 합니다.
- 데이터의 복잡도가 높아지면 높은 차원이 필요하고 데이터의 분포는 단순히 선형 형태가 아닌 비선형 형태를 가지게 됩니다.
- 이러한 분포를 가지는 경우 1차식인 Linear한 형태로 데이터를 표현할 수 없기 때문에 Non-Linearity가 중요하게 됩니다.

## ReLU로 어떻게 곡선 함수를 근사하나?
- y=x 부분인 선형 부분과 비선형 부분 y=0이 결합된 모양입니다. ReLU가 반복 적용 되면서 선형 부분의 결합이 결합이 이루어지고 곡선 합수를 근사할 수 있게 된다.

## ReLU의 문제점
- ReLU의 문제점은 입력값이 0보다 작을 떄, 함수 미분값이 0이 되는 단점이 있다.

## Bias는 왜?
- Bias는 모델이 데이터에 잘 Fitting 하기 위해 평행 이동하는 역할을 합니다.
- 데이터를 2차원으로 표현 했을때, 모든 데이터가 원점을 기준으로 분포해 있지는 않습니다.
- Bias를 이용하여 모델이 평면상에서 이동할 수 있도록 하고 데이터에 더 잘 fitting 될수 있도록 Bias또한 학습 합니다. 

## Sigmoid의 문제점
- 입력 값이 일정 범위를 넘어 가게 되면 0또는 1로 수렵하게 됩니다.
- 그렇게 되면 기울기 또한 0으로 수렴하여 학습 이제대로 되지 않습니다.


# Gradient Descent란?
- 기울기를 통해 함수의 해당 좌표에서 기울기를 통해 최대값이나 최소값을 찾아갈수 있습니다.
- Loss 함수를 최소로 만들기 위해 해당 기울기에 Learning Rate를 곱한 만큼 변화 하면서 최소하 할수 있습니다.

## 왜 Gradient Descent 에서는 기울기를 사용하는가?
- 접선의 기울기를 알면 어느 방향으로 움직여야 함수값이 증가/감소 하는지 알수 있다. 
- 이를 이용해서 비용함수의 최소값을 찾을수 있다.
- gradient를 통해 계산된 Loss를 줄이기 위해 사용한다. 또한 그 차이만큼 Weight를 조정 한다.

## GD 중이 Loss가 증가 하는 이유?
- Optimization 전략에 따라 Gradient가 양수인 방향으로도 Parameter가 증가 할 수 있다. 이 경우 Loss가 일시적으로 증가할 수 있다.

## 중학생에게 설명한다면
- 높은 산에서 앞이 보이지 않는 밤이 됬을 때 하산을 하기 위해서 밑으로 갈수 있는 방법은 현재 내위치의 방향을 토대로 가장 낮은 지점을 찾아갈 수 있다. 

## Back Propagation에 대해서 설명 한다면
- 실제 값과 예측값이 얼마나 차이가 나는지 구하고 그 오차값으 다시 뒤로 전파애 가면서 노드가 가지는 파라미터를 업데이트하는 알고리즘이다.
- 신경망의 최종 단계에서 계산된 오차 변화량을 바탕으로 이전 단계의 파라미터를 업데이트 하는 방향을 설정하는 방법이다.
- 이떄 연쇄법칙으 이용하여 앞단의 기울기 변화량을 통해 뒷쪽으로 전파할 기울기 변화량을 계산할 수 있습니다.

# Local Minima 문제에도 불구하고 딥러닝이 잘되는 이유는?
- Optimization 전략으로 Local Minma 문제를 어느정도 피할수 있기 때문이다.

## GD가 Local Minima문제를 피하는 방법은?
- Momentum 개념을 도입한 RMSprop나 Adam등의 Optimzation 전략을 사용한다.

## 찾은 해가 GLobal Minimum인지 알 수 있는 방법은
- 수식적인 방법은 존재하지 않으나 Test Set을 이용하여 성능을 평가하여 학습된 모델이 Local Minimum에 가까운지 유추할 수있다.


# CNN에 대해서 아는대로 얘기하라
- Convolution 연산을 수행하는 Neural Network입니다.
- 하나의 이미지로부터 픽셀간의 연관성을 살린 여러가지 Feature Vector를 생성하는 과정의 반복 입니다.
- 이떄 Feature Vector를 뽑아내기 위해 Kernel을 사용하고 Kernel을 학습하는 방식으로 이미지를 더 잘 표현할 수 있는 kernel을 학습하여 최종 나타난 Feature Vector를 통해 이미지를 분류 및 다양항 Task에 이용합니다.

## CNN의 파라미터 개수를 계산해라
- 필터의 크기가 WxHxC 이고 개수가 N개라면 총 NxWxHxC개의 파라미터 개수가 필요하다.

# Training Set과 Test Set을 분리 하는 이유
- Training Set을 학습 하기 위해 사용하는 데이터 셋으로 학습의 일반화나 정확도를 판단하기 위해 학습에 관여하지 않는 Test Set과 분리 합니다.

## Validation Set이 있는 이유
- 매 학습 마다 Training Set의 학습 정도를 파악하고 학습시 Overfitting을 찾아내기 위해 사용합니다.

## Test Set이 오염 됬다는 의미는?
- Training set과 너무 유사하여 Test Data가 일반적인 상황을 재현하지 못하여 성능평가를 수행하지 못하는 의미 입니다.

## Regularization
- Cost Function값이 작아지는 방향으로 학습하다 보면 특정 가중치가 너무 커지는 현상이 나타난다. 이떄문에 Overfitting 문제가 발생할 수 있는데 특정 가중치가 너무 커지는 것을 방지 하기 위해 Cost Function 을 계산할 때 가중치의 절대값 만큼 더해주는 방법으로 특정 가중치값이 너무 커지는것을 방지하는 것이다.

# Bath Normalization 효과는?
- 은닉층의 입력값을 Normalization 함으로써 입력 분포를 조정할 수 있다. 
- 은닉층의 입력 분포가 저정되면 학습 편의성이 개선되어 수렴속도가 빨라지고, Local Optima 를 피할 가능성이 높다.

## Drop Out이란
- 의도적으로 특정 파라미터를 학습하지 않는 방법이다.
- 이 방법을 통해 특정 파라미터가 커지는 것을 방지한다.
- 이를 통해 모델의 일반화 성능을 높이고 Overfitting을 방지할 수 있다.

## BN을 적용해서 학습 이후 실제 사용시에 주의할 점은
- BN을 사용할시 학습시에는 입력 데이터에 대해 Normalization을 수행후 연산을 수행한다.
- 그러나 추론 상황에서는 학습시 사용한 배치 단위보다 작을 수 있기 때문에 학습시 사용했던 입력 파라미터의 평균과 분산을 저장해 두고 그 것들의 평균을 통해 normalization을 수행 해야 한다.

## GAN에서 BN이란
- 일반적으로 GAN에서는 BN을 사용하지 않는다. Discriminator가 조작되지 않는 Generator의 결과물을 정확한 값으로 판단하기 위해서 이다.

# SGD, RMSprop, Adam에 대해 설명하시오
- 모두 기울기를 통해 파라미터를 갱신하는 방법이다.

## SGD
- SGD는 파라미터의 기울기와 Learning Rate를 곱한 것을 빼 Loss Functino이 작아지는 쪽으로 다음 파라미터를 결정한다.

## RMSprop
- RMSprop 방식은 이전 변화량이 크다면 현재 변화는 그전 변화량보다 더 적게 변화하는 방식입니다.

## Adam
- 모멘텀과 RMSprop 방식이 적용된 방법입니다.
- 모멘텀을 통해 그전 기울기와 현재 기울기 방향이 일치하다면 더 크게 변화 하고 
- RMSprop 방식은 이전 변화량이 크다면 현재 변화는 그전 변화량보다 더 적게 변화하는 방식으로
- 이 두개의 방식을 같이 적용한 방식이 Adam이다.

## Stochastic Gradient Descent
- Train Dataset을 전체를 이용하는 것이 아닌 mini-batch를 통해 파라미터를 업데이트 한는 것을 의미한다.

# 딥러닝에서 GPU를 쓰면 좋은점
- GPU는 부동소수점 계산에 특화된 많음 코어가 있어, Matrix 연산이 핵심인 Deep Learning에서 병렬 처리 하기 유리 하다.

## 학습 중인데 GPU를 100%사용하지 않는다면 이유는?
- Batch 사이즈가 GPU 메모리에 비해 너무 작기 때문이다.
- 병렬 학습시 output을 한 GPU로 모아서 Loss 계산을 하기 때문에 GPU 메모리에 불균형이 발생할 수 있다. 

## 병럴처리 방법
- 모델과 데이터 처리를 multi-deive에 분배하게 설정할 수 있다. 

## 학습시 필요한 GPU 메모리를 어떻게 계산 하는가?
- GPU 사용량의 미치는 요소는 모델의 Weight 와 Data Tensor이다. 
- Training에 사용되는 Parameter의 개수에 4byte(FP16) 또는 8byte(FP32)를 곱하면 모델의 GPU 메모리를 알 수 있다. 거기에 Input data의 Tensor Size를 더하면 된다


# 평가
![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99DC064C5BE056CE10)
- True Positive : 실제 True인 정답을 True라고 예측(정답)
- False Positive : 실제 False인 정답을 True라고 예측 (오답)
- False Negative : 실제 True인 정답을 False라고 예측 (오답)
- True Negative : 실제 False인 정답을 False라고 예측 (정답)

## Accuracy
- 전체 정답률
- class의 밸런스가 맞지 않을때는 accuracy 
- Imbalance class에서는 Accuracy 기준으로 판단하면 오류가 있기 때문에 Precision과 Recall로 판단 해야한다.

## Precision
- True라고 분류된 것들중 실제 True로 값들의 비율
- 내가 예측한것 들 중이 진짜 인 것들
- TP / (TP + FP)
- FP를 낮추는 데 초점

## Recall
- 실제 True 인 것중이 True라고 예측된 값들의 비율
- 실제 진짜 중에 내가 얼마나 검출 했나?
- TP / (TP + FN)
- 진짜 결함을 정상으로 판단하게 되어 영향일 더 크게 주는 경우

## F1 Score
- Data가 Imbalanced 할때, 정확도가 아닌 F1 Score를 사용한다.
- F1-Score = (2*Precision*Recall) / (Precision + Recall)
![image](https://t1.daumcdn.net/cfile/tistory/99632A415E8BE0F53F)

## Pixel Accuracy
- Pixel 별로 정확도를 검출
- 배경이 많은 이미지에서 전부 0으로 검출했을경우 accuracy가 높아지는 문제점이 있다.

## mIou
- (GT 영역 n 예측 영역) / (GT 영역 U 예측 영역) 

# 선호하는 Libary는?
- pytorch, 상속을 기반으로 모델 설계를 쉽게 할 수 있으며 텐서 컨트롤이 직관적이기 때문이다.

# GAP
- Fully Connected 를 통해서 Featurer를 벡터로 만들면 위치적 정보가 손실된다.
- 채널별로 Average Pooling을 통해 Feature를 뽑으면 위치적 정보를 포함하는 Feature를 뽑을 수 있다.
- 이를 통해 Fully Connected 계층을 없앨수 있고 Fully Connected 계층보다 파라미터를 줄일 수 있다.(GAP 은 파라미터가 없다.)

# CAM
- N개의 클래스를 예측 할때 GAP를 하기전 Feature Map의 채널은 N개 이다.
- GAP에서 나온 결과를 각 Class 별로 예측할때 n개의 가중치가 생셩 된다.
- 이 가중치와 GAP 전 Convolution 계층을 Weighted Sum 을해 해당 클래스의 Heat Map을 나타낼 수 있고 중요도를 표시할 수 있다.

