{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMw6smIFFhJ5ev2ThrRL0kq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# !pip3 install torch\n","# !pip3 install torchvision\n","# !pip3 install matplotlib\n","# !pip3 install torchmetrics\n","\n","import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision\n","from torchvision import datasets\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","print(\"Success\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-EQeXpWRA2-F","executionInfo":{"status":"ok","timestamp":1662286801216,"user_tz":-540,"elapsed":3066,"user":{"displayName":"Dae-won Choi","userId":"01176682398277072949"}},"outputId":"efbb2da1-5465-485f-fab5-b421c090dcf1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Success\n"]}]},{"cell_type":"markdown","source":["## Sigmoid 문제점\n","- 작거나, 큰 영역에서 0에 가까운 작은 값이 나온다. \n","- Backpropagaion시 Activation 함수에서 Gradient를 곱하게 되는데 이떄 Gradient가 0에 가까우면 Gradient가 소멸 되게 된다. => Gradient Vanishing이라 함\n","\n","## ReLU\n","- f(x) = max(0, x)\n","- x가 0보다 큰 영역에서는 gradient가 1이기 때문에 vanishing 문제가 없다.\n","- 0보다 작은 영역에서는 Gradinet가 1이기 떄문에 Vanishing이 나타나지만 사용하기에는 문제 없다."],"metadata":{"id":"BrR0KJcgAzb1"}},{"cell_type":"markdown","source":["## Weight Initialization\n","- 0으로 초기화 하면 모든 Gradient가 0이 되서 학습이 되지 않는다.\n","\n","#### Xavier\n","- 평균이 0, 표준편차가 : V 로 Weight를 초기화 한다.\n","- v = sqrt((2 / (input개수 + ouput 개수)))\n","\n","#### He\n","- 평균이 0, 표준편차가 : V 로 Weight를 초기화 한다.\n","- v = sqrt((2 / input개수))\n","\n"],"metadata":{"id":"u52Za0QhClgQ"}},{"cell_type":"code","source":["class Network(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.linear1 = nn.Linear(786, 256)\n","    self.linear2 = nn.Linear(256, 256)\n","    self.linear3 = nn.Linear(256, 10)\n","    self._weight_init()\n","\n","  def _weight_init(self):\n","    nn.init.xavier_normal_(self.linear1.weight)\n","    nn.init.xavier_normal_(self.linear2.weight)\n","    nn.init.xavier_normal_(self.linear3.weight)\n","\n","model = Network()\n","\n","print(model.linear1.weight)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nD89d4tiEZQE","executionInfo":{"status":"ok","timestamp":1662287894076,"user_tz":-540,"elapsed":261,"user":{"displayName":"Dae-won Choi","userId":"01176682398277072949"}},"outputId":"4a68effb-9be5-41a5-cf33-3d8f94903d89"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[-0.0144, -0.0651, -0.0699,  ...,  0.0174, -0.0781, -0.0704],\n","        [ 0.0519,  0.0425, -0.0111,  ..., -0.0017,  0.0058,  0.0006],\n","        [-0.0818, -0.0162,  0.0302,  ...,  0.0191,  0.0877,  0.0150],\n","        ...,\n","        [-0.0730,  0.0114, -0.0425,  ..., -0.0184,  0.0456, -0.0233],\n","        [ 0.1760,  0.0049, -0.0735,  ...,  0.0304,  0.0310,  0.0458],\n","        [-0.0043,  0.0074, -0.0394,  ...,  0.0100,  0.0594,  0.0064]],\n","       requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["## Overfitting\n","- Train 데이터 셋은 높은 정확도를 가지나, Test 데이터 셋(일반화 상황)에서는 낮은 정확도를 갖는 경우\n","\n","## Dropout\n","- 학습시 각 레이어의 노드를 무작위로 설정된 비율만큼 학습을 시키지 않는 것\n","- overfitting을 방지 할 수 있다.\n","- 매번 다른 네트워크를 사용하여 학습 한다고 생각하여 네트워크 앙상블 효과를 얻을 수 있다. \n","- 추론시에는 학습된 만큼 노드에 가중치를 줘서 학습이 많이된 노드에 가중치를 높인다."],"metadata":{"id":"hVjPCpJhFeqW"}},{"cell_type":"code","source":["class Network(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.p = 0.8\n","    self.layer = nn.Sequential(\n","        nn.Linear(789, 256),\n","        nn.ReLU(),\n","        nn.Dropout(self.p),\n","        nn.Linear(789, 256),\n","        nn.ReLU(),\n","        nn.Dropout(self.p),\n","        nn.Linear(256, 10),\n","        nn.Softmax()\n","    )\n","\n","    self._weight_init()\n","\n","\n","\n","model = Network()"],"metadata":{"id":"aFuv8kSuGtzQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Gradient Vanishing\n","- Backpropagation 시 Gradient가 0에 가까워져 앞단 Layer에 Gradinet가 전달되지 않는 문제\n","- activation function을 sigmoid나 계단 함수를 사용하지 않고 relu나 tanh르 사용한다.\n","- 가중치 초기화 기법을 통해 weight가 0이나 1로 쏠리는 현상을 방지 한다. \n","\n","## Gradinet Exploding\n","- Gradinet가 너무 커져 앞단으로 갈 수록 Weight가 너무크게 갱신된다.(발산)\n","- learning rate를 줄여 발산 현상을 감소 시킨다.\n","\n","## Batch Normalization\n","- Gradinet 문제 해결, 학습 과정이 안정적으로 된다, 학습 속도가 향상 된다.\n","- 첫 8개에 대한 Batch normalization과 첫8개중 n개만 바뀐 데이터의 Batch normalization의 결과가 다를 수 있다는 문제 점이 있다.\n","- 따라서, 추론시 batch가 달랐을때 같은 데이터를 다르게 에측하는 문제가 있을 수 있다.\n","- 학습시 사용된 mean과 variance를 따로 저장 해 두고 추론시에는 저장해둔 mean 과 variance의 평균을 사용 한다. 즉 추론시에는 mean과 variance를 고정하고 추론한다.\n","따라서 Batch-size가 변화해도 mean, variance가 변경되는 문제가 사라진다.\n","\n","#### Internal Covariate Shift\n","- 원래 Input에 데이터 분포에 비해 Layer를 통과 하면서 데이터 분포가 너무 달라지는 문제\n","- 이 데이터 분포가 달라지는 문제를 Batch Normalization을 사용한다. \n","- 각 Layer의 연산이 끝난 후 데이터를 Normalization 한다.\n","- 첫 입력만 Normalization 하는 것이 아닌 Layer마다 Internal Covariate Shift 문제가 발생하기 때문에 각 Layer마다 Normalization 을 한다."],"metadata":{"id":"NpxsU2ZDHbCl"}}]}