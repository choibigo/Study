# 4.1 데이터에서 학습 한다.
- 인공지능 : 사람이 수행하는 지능적인 작업을 자동화 하기 위한 영역
- 기계학습 : 데이터를 분석하고 해당 데이터를 통해 학습 한 후 정보를 바탕으로 결정을 내리기 위해 학습한 내용을 적용하는 알고리즘, 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 '사람' 이 설계한다.
- 딥러닝 : 머신러닝 알고리즘 중 인공 신경망을 기반으로 한 방법들을 통칭한다.

## 4.1.2 훈련 데이터와 시험 데이터
- Training Set : 학습을 통해 최적의 매개변수를 찾을때 사용하는 데이터 
- Validation Set : 모델의 성능을 측정하기 위해 사용, 어떤 모델이 가장 데이터에 적합한지 찾기 위해서 validation set으로 가장 성능이 좋았던 모델을 선택한다. 
- Test Set : 학습된 모델의 정확도를 평가 할떄 사용되는 데이터
- Validation vs Test : validation은 모델들 각각에 적용되어 성능을 측정하며, 최종 모델을 선정하기 위해 사용된다. Test set은 최종 모델에 대해 단 한번만 성능을 측정하며, 앞으로 기대되는 성능을 예측하기 위해 사용한다.
- 범용 능력을 제대로 평가하기 위해 훈련 데이터와 시험 데이터를 분리한다.
- 훈련 데이터에 대해서 정확도가 높으나 시험 데이터에 정확도가 낮다면 **오버비팅** 이라고 한다.
![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fk4UQW%2FbtqLjsEmxTP%2FUGalfLDhyaw5MF5GD5KjI1%2Fimg.png)


# 4.2 손실 함수
- 신경망 학습에서 현재의 상태를 "하나의 지표"를 기준으로 최적의 매개 변수 값을 탐색한다.
- 신경망 학습에서 사용하는 지표는 **손실 함수** 라고 한다.
- 이 손실함수는 일반적으로 오차제곱합과 교차 엔트로피 오차를 사용한다.

## 4.2.1 오차제곱합

#### 수식
![image](https://velog.velcdn.com/images%2Fjakeseo_me%2Fpost%2F1f308ef9-a787-44b2-b7d1-31753e849591%2FMSE.png)
- 오차가 음수나 양수가 나오면 상쇄 될 수 있기 때문에 제곱을 통해 양수의 오차만 연산해 정확한 오차 수치를 알 수 있다.
- y는 신경망의 출력(신경망 추정 값)
- t는 정답 레이블
- k는 데이터의 차원수

```python
def sum_squares_error(y, t):
    0.5 * np.sum( (y-t)**2 )
```

#### 예시
- y : [0.1, 0.03, 0.4, 0, 0] (소프트맥수 함수의 출력이 해당 한다.)
- t : [0  ,    0,   1, 0, 0] (정답 데이터를 원-핫 잍코딩 방식으로 표기)

## 4.2.2 교차 엔트로피 오차
![image](https://velog.velcdn.com/images%2Fjakeseo_me%2Fpost%2F0e182974-46dc-42bd-bd74-146f7dd7737f%2FCEE.png)
- y는 신경망의 출력(신경망 추정 값)
- t는 정답 레이블
- k는 차원수
- t는 원-핫 인코딩 방식으로, 실제로는 정답 레이블의 확률 값이 log를 계산하는 식이 된다.
- 1을 1이라 예측한 경우 -log(1) 로 0에 가까워 진다.
- 1을 0이라 에측한 경우 -log(0)으로 무한대에 가까워 진다. 

```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))

    # log(0) 을 입력하면 마이너스 무한대가 되기 떄문에 이를 방지하기 위해
    # delta를 더해 준다.
```

## 4.2.3 미니배치 학습
- 훈련데이터에 대한 손실 함수 값을 구하고, 그 값을 최대한 줄여주는 매개 변수를 찾아야 한다.
- 이렇게 하려면 모든 훈련 데이터에 대해 손실함수 값을 구해야 한다.
- 만약 훈련 데이터가 100개라면 100개의 손실 함수 값들의 합을 지표로 삼아야 한다.

#### Batch Size를 크게 한 경우
- batch size가 큰 경우 training set 분포를 좀더 근사화 하여 추정할 수 있다.
- noise를 감소 시켜 모델 학습시 안정적인 수렴이 될 수 있다.
- 그러나 실제와 유사하게 근사된 기울기의 절대 값이 작아 수렴 속도가 매우 느려 질 수 있고, 극단적인 경우 극소점이나 안장점에 빠져 로스가 줄어들지 않을 수 있다.
- GPU 메모리를 최대한 활용하기 때문에 학습 속도가 향상 된다.

#### Batch Size를 작게 한 겨우
- sample 수가 적어 데이터에 Noise가 많아  불안정 하게 수렴 된다.
- 그러나 Noise로 인해 Overfitting을 방지하여 모델 성능의 향상의 계기가 될 수 있다.
- Batch size를 작게 하면 Noise가 많아 지고 Regularization 효과를 줄 수 있다.

#### 수식
![image](https://mblogthumb-phinf.pstatic.net/MjAxODA2MTVfMjgg/MDAxNTI5MDQ5MjM2NTE3.RXlKnJudq9HYwqsMmPalt0iVT6utwQFveRPo0dzmtB4g.ShvFHltYpI2fAq5wG7rcxFcNJECZl_QEYgBWyZ3WR-Qg.PNG.ssdyka/e_4.3.png?type=w2)

- 데이터가 N개 라면 tnk는 n번째 데이터의 k번째 예측 값을 의미 한다.
- 마지막으로 N개의 평균 값으로 나누어 "평균 손실 함수"를 구한다.
- 신경망 학습에서 훈련 데이터로부터 일부만 골라 학습을 수행한다. 이를 **미니 배치** fkrh gksek.
- 60000장 데이터중 100장을 무작위로 뽑아 그 100 장만 사용하여 학습하는 방법을 미니 배치 라고 한다.


## 4.2.5 왜 손실 함수를 설정 하는가?
- 우리의 목적은 "정확도"를 높이기 위한 매개변수를 찾는 것이다.
- 그러나 우리는 왜 "정확도"라는 지표를 두고 "손실 함수"와 같은 우회적인 방법을 사용 할까
- 매개변수를 탐색할 때 손실 함수의 값을 가능한 적가 하는 매개 변수 값을 찾는다.
- 이때 매개변수의 미분을 계산하고, 미분을 통해 매개변수가 손실 함수에 미치는 영항도를 파악해 매개변수를 조정 한다.
- 이러한 과정을 반복해 학습을 수행 한다.

- "정확도"를 지표로 삼으면 미분 값이 대부분의 장소에서 0 이 되기 때문에 매개변수를 갱신 할 수 없기 떄문이다.
- 가중치 매개변수의 손실 함수의 미분이란 가중치 매개 변수의 값을 아주 조금 변화 시켰을 때, 손실 함수가 어떻게 변화 하냐 라는 의미이다.
- 그러나 정확도를 기준으로 삼았을 때는 각 가중치에서 조금의 변화는 손실함수에 영향을 줄수 없어 대부분 0이 된다.
- 미분 값이 0 이면 가중치가 손실함수에 미치는 영향은 없다는 뜻이고 더 이상 매개변수를 갱신하지 않는다.

#### 정확도가 지표인 경우
- 100장 훈련 데이터중 32장을 올바로 인식한다면, 정확도는 32%이다.
- 정확도가 지표인 경우는 매개변수 값이 조금 바뀐다 하더라도 32%일 것이다.
- 매개변수 값이 변화해도 정확도(지표)는 개선되지 않거나 불연속 적(33%, 34%)으로 변한다.
- 즉 매개변수의 변화에 따라 지표는 변화가 없거나(미분 값이 0) 불연속적으로 값이 변한다. 그렇기 때문에 신경망 학습이 더 이상 이뤄지지 않는다
- 같은 이유로 활성화 함수를 계단 함수를 사용하지 않는다.

# 4.3 수치 미분
- 경사법에서는 기울기(경사) 값을 기준으로 나아갈 방향을 정한다.

## 4.3.3 편미분
- 변수가 여럿인 함수에 대한 미분을 **편미분**이라 한다.
- 1개의 변수에 대해 편미분을 구할 때 나머지 변수는 상수취급을 하고 미분한다.


## 4.4 기울기
- ![image](https://velog.velcdn.com/images%2Fkyj93790%2Fpost%2Fc86d6345-2edf-432a-af64-a7f365b1601e%2Fimage.png)

- 기울기는 가장 낮은 장소를 가르키나, 반드시 그렇다고는 할 수 없다.
- 기울기는 현재 자신의 위치에서 낮아지는 방향을 가리킨다.
- 손실 함수의 식에서 가장 낮은 위치에 의미한다는 것은 오차가 가장 작다는 것을 의미 한다.
- 그렇기 때문에 미분을 통해 가장 낮은 방향으로 매개변수를 변화 하면 학습을 진행 할 수 있다.

## 4.4.1 경사법(경사 하강법)
- 신경망은 최적의 매개변수를 찾아야 한다.
- 최적의 매개 변수란 손실함수가 최소값이 될 때의 매개변수 값이다.
- 손실함수의 기울 기를 이용해 최소값을 찾는 것이 경사 하강법이다.
- 기우러진 방향이 꼭 최소값은 아니나, 그 방향으로 가야 함수의 값을 줄일 수 있다.
- 경사 하강법은 현 위치에서 기울어진 방향으로 일정 거리(Learning Rate) 만큼 이동한다.
- 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기 반복한다.
- 이렇게 해서 함수의 값을 점차 줄이는 것을 경사 하강 법이라고 한다.

![image](https://velog.velcdn.com/images%2Fkyj93790%2Fpost%2Fe0d7b3d3-a136-4fec-b5cd-dab1987f616b%2Fimage.png)

- 경사법으 사용한 갱신은 위 그림과 같다
- 값이 높은 위치에서 낮은 위치로 서서히 변하게 된다.
- 이떄 한번 변할떄 얼만큼 변하냐의 척도는 Learning Rate이 결정한다.
- Learning Rate이 너무 크다면 최소점을 찾지 못해 발산하고
- Learning Rate이 너무 작다면 거의 갱신하지 못한채 학습이 끝난다.
``
하이퍼 파라미터
- 사람이 직접 설정해야하는 매개 변수이다.
- Epoch, Batch Size, Learning Rate ... 
``

## 신경망에서의 기울기
![image](https://velog.velcdn.com/images%2Fkyj93790%2Fpost%2F8e768d3a-3114-4bb1-b776-e3ee929d10d9%2Fimage.png)
- 각 미분 값은 해당 Weight를 조정했을 때 손실함수 L에 대해서 미치는 영향을 나타 낸다.
- 만약 미분값이 -0.5라면 h만큼 변화 했을때 손실 함수 L은 0.5h만큼 감소한다는 의미이다.  