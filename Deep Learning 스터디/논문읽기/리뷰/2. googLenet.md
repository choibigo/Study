# 등장 배경
- Networkd 깊이와 넓이가 증가할 수록 높은 정확도를 얻을 수 있지만 두가지 단점이 있다.
- 첫 번째 기하 급수적으로 Parameter가 증가 한다. Paramter 수가 많아 지면 Overfitting하기 쉬워지고 Bottleneck을 만들어 낸다.
- 두번째 메모리 사용량이 증가 한다.

# Motivation and High Level Considerations
- 신경망의 성능 개선을 위해 간단한 방법은 신경망 크기를 늘리는 것이다.
- 크기가 커질 수록 파라미터의 수가 늘어나고 오버피팅이 일어나기 쉬워지고 병목현상? 이 일어날 수 있다.
- 네트워크가 커질수록 컴퓨터의 자원의 사용량이 늘어난다.
  
![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FBGbXV%2FbtqSDvAa5N1%2FbVaRLW8gpuO5ymxKZfHkbK%2Fimg.png)
- 이를 해결하기 위해 Dense 한 Fully-Connected 구조에서 Sparsely Connected 구조로 바꾼다.
- dataset의 분포를 Sparse 하면서도 더큰 심층 신경망으로 표현가능하다면, 입력 layer에서 출력 layer로 향하는 layer 간의 관계를 통계적으로 분석한 후, 연관 관계가 높은 것들만 연결하여 최적의 Sparse한 네트워크를 만들 수 있다.


# Inception Module
- Inception 구조의 주요 아이디어는 CNN에서 각 요소를 최적의 Local Sparce Structure로 근사화 하고, 이를 Dense Component로 바꾸는 방법을 찾는 것이다.
- Sparse 매트릭스를 서로 묶어 상대적으로 Dense한 Submatrix를 만드는 것이다.


![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbKbnNV%2FbtqSxB8J5jA%2F2CjFDvUTU9wIKJ1NSNBWy0%2Fimg.png)
- layer당 Highly Correlated 된 Units끼리 Clustering 하고, 이 Cluster들이 다음 layer의 Units이 된다.
- 입력이미지와 가까운 낮은 layer에서는 특정 부분에 Correlated Unit들이 집중되어 있다. 
- 단일 지역에 많은 클러스터들이 집중된다는 뜻이기에 1x1 Convolution으로 처리할 수 있다.
- 깊은 Layer 일수록 더 넓은 영역의 Convolution Filter가 있어야 Correlated Unit의 비율을 높일수 있다. 그렇기 대문에 1x1, 3x3, 5x3, Convolution 연산을 병렬적으로 수행한다.

- ![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FpxaiH%2FbtqRt531KT7%2FFhclQVjlJX3k7L6xNJnoW1%2Fimg.png)
- Network에서 최적의 Local Sparse Structure를 구성하고, 어떻게 Components로 구성할 수 있을지를 알아내는 것이다.
- 네트워크는 Convolutional Block을 쌓아서 구성된다. 즉, 최적의 Local Construction을 찾고 그것을 반복하면 된다.
- 무턱대고 네트워크를 늘리면 Overffting 연산과 계산량이 크게 증가한다는 단점이 있기 때문에 네트워크를 **Sparse**하게 구성하여 네트워크를 증가 시킨다.
- Sparse한 Structure는 하드웨어 계산에 비효율 적이기 때문에 Sparse Structure를 다시 Dense한 Component들로 구성한다.
- feature map을 효과적으로 추출하기 위해 1x1, 3x3, 5x3의 Convolution 연산을 각각 수행한다.
- 특징 정보가 다양한 규모로 처리되고 다음 계층은 동시에 서로 다른 규모에서 특징을 추출할 수 있게 된다.
-  Visual 정보가 다양한 Scale로 처리되고, 다음 layer는 동시에 서로 다른 layer에서 특징을 추출할 수 있다. 1 x 1, 3 x 3, 5 x 5 Convolution 연산을 통해 다양한 특징을 추출할 수 있기 때문이다.
- 1x1 convolution을 통한 차원 축소로 3x3과 5x5 Convolution 전에 연상량을 감소 시켰다.

# Architectural Details
- ![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FcGlsra%2FbtqSjAXtdln%2Fcy32Ke5qRU9vq0ERLrHAF0%2Fimg.png)
- '# 3 x 3 reduce와 '# 5 x 5 reduce는 3 x 3과 5 x 5 Convolutional layer 앞에 사용되는 1 x 1 필터의 채널 수를 의미한다. 그리고 pool proj 열은 max pooling layer 뒤에 오는 1 x 1 필터의 채널 수를 의미한다. 여기서 모든 reduction 및 projection layer에 ReLU가 사용된다.

## Part1
- ![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbiTAD1%2FbtqSELQkwgt%2FkJPKmvCoM19ph9Jon1zGb0%2Fimg.png)
- 입력 이미지와 가까운 낮은 레이어에서는 효율을 위해 기본적으로 CNN 모델을 적용한다.
- 높은 레이어에서만 Inception Module이 사용된다.

## Part2
- ![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbTGs3P%2FbtqSxBVdZ5u%2F3cyfHeBiEGEScc5zdpYnl1%2Fimg.png)
- Inception module로서 다양한 특징을 추출하기 위해 1 x 1, 3 x 3, 5 x 5 Convolutional layer가 병렬적으로 연산을 수행하고 있으며, 차원을 축소하여 연산량을 줄이기 위해 1 x 1 Convolutional layer가 적용되어 있는 것을 확인할 수 있다.

## Part3
- ![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fb0wn0v%2FbtqSmAiDErs%2FoTQTSFOERoyEwPDs2u0alk%2Fimg.png)
- Auxiliary Classifier 가 적용된 부분이다.
- 모델의 깊이가 깊어질 수록 Gradient Vanishing 문제가 발생할 수 있으므로 신경망 중간 layer에서 Auxiliary Classifier를 추가하여 중간군간 결과를 출력해 역전파를 일으켜 gradient가 전달될 수 있게끔 한다.
- 지나치게 영향을 주는 것을 막기 위해 Auxsiliary Classifier의 loss에 0.3을 곱하고 실제 Inference에는 제일 끝단의 Softmax만 사용한다.

## Part4
- ![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FIwjPt%2FbtqSGjTyGu5%2FzBa3eJ0oegcjcBQk3EVz3K%2Fimg.png)
- 최종 Classifier 이전에 Average Pooling Layer를 사용하여 Feature Map을 1차원 벡터로 만들어 준다.
- 1차원 벡터로 만들어줘야 최종적으로 이미지 분류를 위한 Softmax layer와 연결할 수 있다.

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FevD2RB%2FbtqSATBmCEB%2FpkrTC9WFiqbkYiqZUxUgq1%2Fimg.png)
- 1차원 벡터로 만들면 가중치 개수를 줄여준다, FC 방식을 이용할 경우 가중치 개수가 51.3M 이지만 GAP를 사용하면 가중치가 필요하지 않다.
- GAP을 적용할 시, Fine Tuning을 하기 쉽게 만든다???

#### 참고 
- https://phil-baek.tistory.com/entry/3-GoogLeNet-Going-deeper-with-convolutions-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0
- https://leedakyeong.tistory.com/entry/%EB%85%BC%EB%AC%B8-GoogleNet-Inception-%EB%A6%AC%EB%B7%B0-Going-deeper-with-convolutions-1