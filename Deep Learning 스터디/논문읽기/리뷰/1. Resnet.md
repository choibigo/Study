## Intorduction
- 신경망은 추상화에 대해 low / mid / high level의 특징을 multi-layer 방식으로 통합한다.
- 각 추상화 level은 쌓인 layer의 수에 따라 높아 진다.
- 그러나 Layer가 많아 질수록  Degradation : (**Vanishing** / **Exploding gradient**) 현상이 생긴다.
- 이를 해결하기 위해 Residual Block을 사용한다.

#### Vanishing Gradient Problem(기울기 소멸 문제)
- Back Propagation에서 계산 결과와 정답과의 오차를 통해 가중치를 수정하는데, 입력층으로 갈수록 기울기가 작아져 가중치들이 업데이트 되지 않아 최적의 모델을 찾을 수 없는 문제이다.
- 은닉층을 많이 거칠 수록 전달되는 오차가 크게 줄어들어 학습이 되지 않는 현상이 발생한다.
- 기울기가 0으로 소멸되어 버리면 네트워크의 학습은 매우 느려지고, 학습이 다 이루어지지 않는 상태에서 멈추게 된다.
- ReLU, Leaky ReLU 활성화 함수를 사용하거나 Batch Normalization 기법을 사용해 이를 해결한다.

#### Batch Normalization
- 일반적으로 출력 계층과 활성화 계층 사이에 존재한다.
- 출력 결과를 평균이 0이 되도록 정규화 한다. (=기울기가 한쪽으로(0이나 1) 쏠리는 현상을 방지 한다.) 

#### Exploding Gradient 
- 기울기가 점차 커져 가중치들이 비정상 적으로 큰 값이 되면서 발산하는 현상
- 기울기 소멸과 같은 방법으로 방지 할 수 있다.

## Residual Block
![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbCq53Y%2FbtqS3aiUBz4%2FKwFH2Eqf3FTWgrYwsmHAy1%2Fimg.png)
![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FQP9is%2FbtqTcQ4s52b%2FARS9MrzwdbhJKRKDhtkWx1%2Fimg.png)
- 기존 네트워크는 입력 X를 받고 Layer를 거쳐 H(x)를 출력한다, 
- 입력값 x를 타겟값 y로 mapping하는 함수 H(x)를 얻는 것이 목적이다.
- Residual Learning은 H(x)가 아닌 출력과 입력의 차인 H(x)-x를 얻도록 목표를 수정한다.
- Residual Function => F(x) = H(x)-x를 최소화 해야 한다, 즉 출력과 입력의 차를 줄인다라는 의미가 된다.
- x의 값은 바꾸지 못하므로 F(x)가 0이 되는 것이 최적의 해이고, 결국 0 = H(x)-x => H(x) = x가 된다, 즉 H(x)를 x로 mapping 하는 것이 학습 목적이 된다.
- 기존에는 알지못하는 H(x)로 학습 시켜야 한다는 점에 어려움이 있었다.(H(x)를 알지 못하는 최적의 값으로 근사 해야 한다. )
- H(x)=x 라는 최적의 목표값이 사전에 정해지기 때문에 Identity Mapping인 F(x)가 학습하기 더 쉬워진다.
- 입력에서 출력으로 바로 연결되는 shortcut만 추가 하면 되기 때문에 파라미터 수에 영향이 없으며, 연산량 증가도 없다.
- 기존 학습 했던 정보를 그대로 가져오고 추가적으로 학습할 정보만 추가적으로 더한다.
- 또한 H(x)를 미분해도 F(x)' +1 이므로 Gradient Vanishing 현상을 해결 했다.
- Shortcut Connection 방식을 이용하면 떨어진 두개의 층을 연결 함으로써 오차 역전파 시 기울기가 쉽게 전파 된다는 장점이 있다.
- 
## Deep Residual Learning

#### Residual Learning
- 복잡한 함수를 다수의 비선형 레이어가 근사시킬 수 있다면 잔차 함수도 근사할 수 있다는 가설이다.
- 이 증명은 실험을 통해 밝혀 졌다.

#### Identity Mapping By Shortcuts
![image](https://k.kakaocdn.net/dn/cI7peb/btqC7qz94i6/LG8TmaohIAZ9KZISrmPS21/img.png)
- x는 Input, y는 output vector, F(x,{Wi})는 학습될 residual mapping을 의미한다.

## Network Architectures
![image](https://t1.daumcdn.net/cfile/tistory/9931D4425CB8A11F1E)
- 처음을 제외하고 모두 3X3 사이즈의 컴볼루션 필터를 사용했다.
- 맵이 반으로 줄 떄, 맴의 뎁스를 2배로 높였다.
- 마지막엔 Global Average Pooling(GAP)을 사용한다.

#### GAP
- Pooling은 필터에 사용된 파라미터 수를 줄여서 차원을 감소시킨다.
- 최대값을 취하지 않고 평균 값을 취하는 방법이 있다, 이 경우를 Average Pooling이라 한다.
- GAP의 목적은 Max Pooling 보다 더 급격하게 Feature의 수를 줄인다.
- 하지만 Pooling의 목적과는 조금 다르다, GAP의 목적은 Feature를 1차원 벡터로 만들기 위함이다.
![image](https://gaussian37.github.io/assets/img/dl/concept/gap/5.png)
- 같은 채널의 feature를 모두 평균을 낸 다음 채널의 개수 만큼 원소를 가지는 벡터로 만든다.
- (height, width, channel) => (channel, ) 형태로 간단히 만들어 버린다.
- GAP은 CNN+FC 구조에서 FC Layer를 없애기 위한 방법으로 도입 됬다.
- FC layer는 마지막 feature와 matrix곱을 하여 feature 전체를 연산의 대상으로 삼아서 결과를 출력 한다. 즉, feature가 이미지 전체를 함축하고 있다고 가정하면 이미지 전체를 보고 출력을 만들어 내는 것이다.
- FC Layer는 파라미터 수가 많이 증가하는 단점과 Feature 전체를 matrix연산하기 때문에 위치에 대한 정보도 사라진다.
- FC Layer 사용시 반드시 지정해 주어야 하는 FC Layer의 사이즈로 인해 입력 이미지 사이즈 또한 그에 맞춰서 고정되여야 한다.
- 어떠한 크기의 feature라도 같은 채널의 값들을 하나의 평균 값으로 대체하기 때문에 벡터가 된다. 따라서 입력 사이즈에 관계가 없다.

#### 참조
- [참조 1](https://phil-baek.tistory.com/entry/ResNet-Deep-Residual-Learning-for-Image-Recognition-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0)
- [참조 2](https://jxnjxn.tistory.com/22)