{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 통계 기반 기법\n",
    "- 말뭉치(Corpus)는 대량의 텍스트 데이터 이다.\n",
    "- 맹목적으로 수집된 텍스트 데이터가 아닌 자연어 처리 연구나 애플리케이션을 염두에 두고 수집된 텍스트 데이터이다.\n",
    "- Corpus에는 자연어에 대한 사람의 '지식'이 충분히 담겨 있다고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 파이썬으로 말뭉치 전처리하기\n",
    "- 다양한 문장이 말뭉치로 사용될 수 있다, 위키백과, 구글 뉴스, 소설 등\n",
    "- 전처리란 텍스트 데이터를 단어로 분할하고 그 분할된 단어들을 단어 ID 목록으로 변환하는 일이다.\n",
    "- '통계 기반 기법'을 이용해 단어를 벡터로 표현할 수 있다, 그 전에 문장을 전처리 해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess(text):\n",
    "    # text => words => id list\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.replace('.',' .')\n",
    "\n",
    "    words = text.split(' ')# 문장을 단어 형태로 분할\n",
    "\n",
    "    # 단어에 ID를 부여하고 ID의 리스트로 이용할 수 있도록 손질한다.\n",
    "    word_to_id = {} # 단어로 ID 찾기\n",
    "    id_to_word = {} # ID로 단어 찾기\n",
    "\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[idx] for idx in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word= preprocess(text)\n",
    "print(corpus)\n",
    "print(word_to_id)\n",
    "print(id_to_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 단어의 분산 표현\n",
    "- 색상을 RGB로 표현한다면 더 정확하게 명시 할 수 있다, 또한 비슷한 색인지 판단 여부도 벡터를 이용한다면 쉽게 판단할 수 있다.\n",
    "- 단어를 벡터로 표현함으로써 '단어의 의미'를 정확하게 나타낼 수 있다, 이를 단어의 '분산 표현'이라 한다.\n",
    "\n",
    "#### 2.3.3 분포 가설\n",
    "- 단어의 의미는 주변 단어에 의해 형성된다, 이를 분포 가설(Distribution Hypotheis)라 한다.\n",
    "- 단어 자체는 의미가 없고, 그 단어가 사용된 '맥락'이 의미를 형성한다는 것이다.\n",
    "- \"We drink wine\", \"We drink beer\"처럼 drink 주변에는 음료가 등장하기 쉬우며 \"We guzzle wine\", \"We guzzle beer\"을 보면 drink와 guzzle이 같은 맥락에서 사용된 것을 볼 수 있다.\n",
    "- 따라서 \"drink\"와 \"guzzle\"은 같은 의미로 파악 할 수 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/ec26ef9b-a540-49b8-a508-9dbc52caeec4)\n",
    "\n",
    "- 주변에 단어가 '맥락'에 해당 된다.\n",
    "- 맥락의 크기를 Window size라고 한다.\n",
    "\n",
    "#### 2.3.4 동시발생 행렬\n",
    "- 분포 가설에 기초해 단어를 벡터로 나타내는 방법으로 '주변 단어를 세어 보는'방법이 자연스럽게 떠오른다.\n",
    "- 어떠 단어에 주목했을 때, 그 주변에 어떤 단어가 몇번이나 등장하는지를 세어 집계하는 방법이다.\n",
    "- 이를 통계 기반 기법이라 한다.\n",
    "- 동시발생 행렬을 이용하면 단어를 벡터로 표현할 수 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/72c1b01c-5402-48cf-812f-2bbbe74e66da)\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/fa40c3d0-8ecc-4782-a794-80b7018c08d4)\n",
    "\n",
    "- You 주변에 단어들을 (맥락) 파악하여 표로 정리할 수 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/619c112e-cc41-40b5-bbba-cc42904e4363)\n",
    "\n",
    "- 모든 ID에 대해서 맥락을 파악할 수 있다, 이를 동시발생 행렬 이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    # corpus: 단어 ID의 리스트, 어휘 수: corpus에서 사용된 단어 수, 윈도우 크기 이다.\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id]+=1\n",
    "            \n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id]+=1\n",
    "            \n",
    "    return co_matrix\n",
    "\n",
    "co_matrix = create_co_matrix(corpus, len(word_to_id), 1)\n",
    "print(co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 벡터 간 유사도\n",
    "- 벡터간 유사도를 나타낼 떄는 코사인 유사도를 이용한다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/cad6bd68-3e34-45b0-942e-9c2dc596d25b)\n",
    "\n",
    "#### Cosine Similarity\n",
    "- 두 벡터의 유사도를 의미하며 동일한 경우 1, 90°의 각을 이루면 0, 180°로 반대 방향을 가지면 -1을 갖는다\n",
    "- 두 벡터가 가리키는 방향이 얼마나 유사한가를 의미한다.\n",
    "- cosin 이 사이의 각도 인데 사이의 각도가 0 에 가까울 수록 방향이 겹치는 것으로 해석할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "def cos_similarity(x, y):\n",
    "    esp = 1e-8\n",
    "    nx = x / ((np.sqrt(np.sum(x ** 2)))+ esp)\n",
    "    ny = y / ((np.sqrt(np.sum(y ** 2)))+ esp)\n",
    "\n",
    "    # 벡터를 정규화 한 이후에 내적을 하여 코사인 유사도를 구했다 (위 식과 같다고함)\n",
    "\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "c0 = co_matrix[word_to_id['you']] # \"you\"의 단어 벡터 - [0 1 0 0 0 0 0]\n",
    "c1 = co_matrix[word_to_id['i']] # \"you\"의 단어 벡터 - [0 1 0 1 0 0 0]\n",
    "\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.6 유사 단어의 랭킹 표시\n",
    "- 어떤 단어가 주어지면, 그 단어와 비슷한 단어를 유사도 순으로 출력하는 함수를 구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 316 is out of bounds for axis 0 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 37\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[39mif\u001b[39;00m top_count \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m top:\n\u001b[0;32m     32\u001b[0m             \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m most_similar(\u001b[39m\"\u001b[39;49m\u001b[39myou\u001b[39;49m\u001b[39m\"\u001b[39;49m, word_to_id, id_to_word, co_matrix)\n\u001b[0;32m     38\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mgoodbye: 0.7071067691154799\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39mi: 0.7071067691154799\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mi와 you가 유사하다는 것은 이해가 가지만 나머지는 이상하다, 이는 corpus가 너무 작아서 나타나는 현상이다.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39m'''\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 16\u001b[0m, in \u001b[0;36mmost_similar\u001b[1;34m(query, word_to_id, id_to_word, word_matrix, top)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m# 검색어 정보\u001b[39;00m\n\u001b[0;32m     15\u001b[0m query_id \u001b[39m=\u001b[39m word_to_id[query]\n\u001b[1;32m---> 16\u001b[0m query_vec \u001b[39m=\u001b[39m word_matrix[query_id]\n\u001b[0;32m     18\u001b[0m \u001b[39m# 모든 벡터에 대해 코사인 유사도 계산\u001b[39;00m\n\u001b[0;32m     19\u001b[0m vocab_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(id_to_word)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 316 is out of bounds for axis 0 with size 7"
     ]
    }
   ],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''\n",
    "    query : 검색어\n",
    "    word_to_id : 단어에서 단어 ID로의 딕션어리\n",
    "    id_to_word : 단어 ID에서 단어로의 딕션어리\n",
    "    word_matrix : 단어 벡터들을 한데 모은 행렬 (여기서는 co_matrix 사용, 여러 기법을 통해 단어 벡터를 나타낼 수 있다봄)\n",
    "    top : 상위 몇개까지 출력할 것인지 결정\n",
    "    '''\n",
    "\n",
    "    if query not in word_to_id:\n",
    "        print(f\"쿼리({query})가 딕셔너리에 없음\")\n",
    "        return\n",
    "    \n",
    "    # 검색어 정보\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 모든 벡터에 대해 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(query_vec, word_matrix[i])\n",
    "    \n",
    "    # 코사인 유사도 기준으로 내림차순 정렬\n",
    "    for top_count, i in enumerate((-1 * similarity).argsort()):\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "\n",
    "        print(f\"{id_to_word[i]}: {similarity[i]}\")\n",
    "        \n",
    "        if top_count >= top:\n",
    "            return\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "most_similar(\"you\", word_to_id, id_to_word, co_matrix)\n",
    "'''\n",
    "goodbye: 0.7071067691154799\n",
    "i: 0.7071067691154799\n",
    "hello: 0.7071067691154799\n",
    "say: 0.0\n",
    "and: 0.0\n",
    "\n",
    "처럼 나타 났다.\n",
    "i와 you가 유사하다는 것은 이해가 가지만 나머지는 이상하다, 이는 corpus가 너무 작아서 나타나는 현상이다.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통계 기반 기법 개선하기\n",
    "- 단어의 동시발생 행렬은 만들었다.\n",
    "- 동시발생 행렬에는 아직 개선할 점이 있다\n",
    "- 이를 통해 더 실용적인 말뭉치를 사용하여 '진짜'단어의 분산 표현을 할 수 있다.\n",
    "\n",
    "#### 2.4.1 상호정보량\n",
    "- 동시발생 행렬의 원소는 두 단어가 동시에(window size내 발생) 발생한 횟수를 나타낸다.\n",
    "- \"발생\" 횟수라는 것은 좋은 특징은 아니다, 고빈도 단어를 보면 바로 확인할 수 있다.\n",
    "- \"the\"와 \"car\"의 동시발생을 생각해 보면 분명 \"... the car...\"라는 문구가 자주 보일 것이다. 따라서 the와 car가 상관 관계가 높다고 판단할 수 있다.\n",
    "- 그러나 실제 언어에서는 the와 car 관게보다 car와 drive 관련이 더 깊다.\n",
    "- 단순히 등장 횟수만 본다면 car는 drive보다 the와의 관련성이 훨씬 높다고 판단 될 수 있다.\n",
    "- 이 문제를 해결하기 위해 점별 상호 정보량(Pointwise Mutual Information)이라는 척도를 사용할 수 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/20340e2b-5c86-4619-bede-7d7f4227068e)\n",
    "\n",
    "- P(x)는 x가 일어날 확률, P(y)는 y가 일어날 확률, P(x, y)는 x와 y가 동시에 일어날 확률을 의미한다.이 PMI 값이 높다면 관련성이 높다는 의미이다.\n",
    "- P(x)는 x가 말뭉치에 등장할 확률을 가리킨다.\n",
    "- 예를 들어 10000개의 단어로 이루어진 말뭉치에 \"the\"가 100번 등장한다면 P(\"the\")는 100/10000 = 0.01이다.\n",
    "- \"the\"와 \"car\"가 10번 동시발생했다면 P(\"the\", \"car\") = 10 / 10000= 0.001이 되는 것이다.\n",
    "- C(x)를 동시 발생 횟수로 했을때 식을 재정의할 수 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/29bd68d7-e8c9-4fd8-9dc9-f9885a6b3021)\n",
    "\n",
    "- 이때 corpus에 포함된 단어의 수가 N이다.(전체 단어의 총 동시발생 횟수)\n",
    "- 총 단어수 : 10000\n",
    "- \"the\" : 1000\n",
    "- \"car\" : 20\n",
    "- \"drive\": 10\n",
    "- \"the\" & \"car\" : 10 \n",
    "- \"car\" & \"drive\" : 5 \n",
    "- 동시발생 횟수 관점에서는 \"car\"는 \"drive\"보다 \"the\"와 관련이 더 깊다고 말할 수 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/4cee6014-8009-4399-b3fc-c7126bb46904)\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/cd2d9e29-7f15-4670-a9e3-9b093f966d2f)\n",
    "\n",
    "- PMI를 사용한다면 \"car\"와 \"drive\"가 더 관련이 깊다고 나온다.\n",
    "- 이러한 이유는 단어가 단독으로 출현하는 횟수가 고려 되었기 때문이다\n",
    "- 반대로 말하면 동시발생 횟수의 문제점은 단어가 단독으로 출현하는 경우를 고려하지 않았기 때문에 성능이 좋지않다.\n",
    "- PMI를 그대로 사용한다면 동시발생횟수가 0 일때 -∞가 된다.\n",
    "- 따라서 양의정보량(Positive PMI: PPMI)를 사용한다.\n",
    "- PPMI(x,y) = max(0, PMI(x,y))\n",
    "- PMI가 음수일 때는 0으로 취급한다.\n",
    "- PPMI의 각 원소는 0 이상이다.\n",
    "- 동시발행 행렬보다 더 좋은 척도가 된다.\n",
    "- PPMI의 큰 문제점은 벡터의 차원이 증가함에 따라 차원의 수도 증가 하게 된다.\n",
    "- 예를 들어 corpus가 10만이라면 벡터의 차원수도 10만이 된다, 10만 차원의 벡터를 다룬다는 것은 현실적이지 않다.\n",
    "- PPMI 행렬을 보면 대부분이 0인것을 알 수 있다 즉, 벡터 원소 대부분이 중요하지 않다는 것이다. 다르게 포현하면 대부분의 원소의 '중요도'가 낮다는 것이다.\n",
    "- 이런 벡터는 노이즈에 약하고 견고하지 못한 약점도 있다.\n",
    "- 이러한 문제를 해결하고자 자주 수행하는 것이 벡터의 차원 감소이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPMI Calculate END\n",
      "[[0.        1.8073549 0.        0.        0.        0.        0.       ]\n",
      " [1.8073549 0.        0.8073549 0.        0.8073549 0.8073549 0.       ]\n",
      " [0.        0.8073549 0.        1.8073549 0.        0.        0.       ]\n",
      " [0.        0.        1.8073549 0.        1.8073549 0.        0.       ]\n",
      " [0.        0.8073549 0.        1.8073549 0.        0.        0.       ]\n",
      " [0.        0.8073549 0.        0.        0.        0.        2.807355 ]\n",
      " [0.        0.        0.        0.        0.        2.807355  0.       ]]\n"
     ]
    }
   ],
   "source": [
    "def ppmi(C, verbose=False):\n",
    "    '''\n",
    "    C: co_matrix\n",
    "    verbose(=말 수가 많은): 진행상황 출력 여부\n",
    "    '''\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i,j] * N / (S[j]*S[i]) + 1e-8)\n",
    "            M[i,j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print(f'{(100*cnt/total):.2f}% 완료')\n",
    "\n",
    "    print(\"PPMI Calculate END\")\n",
    "\n",
    "    return M\n",
    "\n",
    "ppmi_matrix = ppmi(co_matrix)\n",
    "\n",
    "print(ppmi_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 차원감소\n",
    "- 차원 감소는 문자 그대로 벡터의 차원을 줄이는 방법을 말한다.\n",
    "- 단순히 차원을 줄이는게 아니라, '중요한 정보'는 최대한 유지하면서 줄이는 게 핵심이다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/e219d7d9-70bf-4eb4-889a-c752b49ea35c)\n",
    "\n",
    "- 위 그림처럼 데이터의 분포를 고려해 중요한 '축'을 찾는 일을 수행한다.\n",
    "- 중요한 축을 찾는 다면 각데이터를 축에 사영 시켜 데이터를 축소할 수 있다. (위그림에서는 1차원 으로 표현할 수 있다.)\n",
    "- 여기서 가장 중요한 것은 데이터를 가장 잘 표현하는 축을 찾는 것이다.\n",
    "- 차원을 감소 시키는 방법은 SVD를 통해할 수 있다.\n",
    "\n",
    "#### 2.4.3 SVD에 의한 차원 감소\n",
    "- SVD는 numpy의 linalg 모듈로 실행 할 수 있다. \n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/3a937dff-6df2-480e-ac2a-fadc20450608)\n",
    "\n",
    "- SVD를 이용하여 임의의 행렬을 세 행렬의 곱으로 분해하여 표현할 수 있다.\n",
    "- U와 V는 직교 행렬이고, 그 열벡터는 서로 직교한다.\n",
    "- S는 대각행렬로 eigen value 의 집합이다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/a49d555e-4f78-4260-928c-ebf130de6f72)\n",
    "\n",
    "- U는 직교 행렬로 이 행렬을 '단어 공간'으로 취급할 수 있다.\n",
    "- egien value순으로 정렬 되어 나오기 때문에 그 순서가 데이터의 미치는 중요도 순으로 나온다.\n",
    "- 중요도가 낮은 벡터를 줄여서 차원을 축소 할 수 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/3d5151da-ea8f-4a6c-bd26-ef4c97dfbea6)\n",
    "\n",
    "- 원래 단어를 SVD 한 이후에 U의 열벡터를 깍아서 (실제로는 s의 원소(eigen value)가 작은 애들을 없애는것) 데이터를 근사할 수 있다.\n",
    "- x의 각 행에는 해당 단어 ID의 단어 벡터가 저장되어 있으며, 그 단어 벡터가 U'라는 차원 감소된 벡터로 표현되는 것이다.\n",
    "\n",
    "#### 희소 표현\n",
    "- 원-핫 벡터로 데이터를 표현하면 단어의 인덱스만 1이고 나머지인덱스는 모두 0이다.\n",
    "- 단어 집합이 클수록 고차원의 벡터가 되고 대부분이 0이다.\n",
    "- 이때 이러한 벡터 표현은 공간적 낭비가 된다.\n",
    "- 이러한 표현 방법을 희소 표현이라하며 이렇게 표현된 벡털르 희소 벡터라고 한다.\n",
    "\n",
    "#### 밀집 벡터\n",
    "- 희소 벡터와 반대되는 개념이다.\n",
    "- 1000개의 단어를 원핫 벡터로 1000차원으로 표현 했을떄 차원 축소를 해 128 차원으로 표현 했다면 모든 벡터의 원소가 실수 값이 된다. 이때 벡터의 차원이 더 조밀해 졌다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        1.8073549 0.        0.        0.        0.        0.       ]\n",
      "[ 3.4094876e-01 -1.1102230e-16 -1.2051624e-01 -3.3306691e-16\n",
      " -9.3232495e-01  0.0000000e+00 -2.4257469e-17]\n",
      "[[ 3.4094876e-01 -1.1102230e-16 -1.2051624e-01 -3.3306691e-16\n",
      "  -9.3232495e-01  0.0000000e+00 -2.4257469e-17]\n",
      " [ 0.0000000e+00 -5.9763640e-01  0.0000000e+00  1.8023790e-01\n",
      "   0.0000000e+00 -7.8124583e-01  0.0000000e+00]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n- goodby와 hello가 가깝고 you와 i가 가까운 것을 확인할 수 있다.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "U, S, V = np.linalg.svd(ppmi_matrix) # ppmi의 결과의 대부분은 0으로 채워져 있기 때문에 차원 축소를 해서 표현하는 것이 효율적이다\n",
    "\n",
    "print(ppmi_matrix[0])\n",
    "print(U[0]) # 0 이었던 값들이 실수 값으로 채워지면서 밀집 벡터가 됬다.\n",
    "print(U[0 :2]) # 차원 축소를 하기 위해 처음 2개의 원소를 꺼내면 된다(Eigen value 순으로 정렬 되있기 때문)\n",
    "\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "\n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()\n",
    "'''\n",
    "- goodby와 hello가 가깝고 you와 i가 가까운 것을 확인할 수 있다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.4 PTB 데이터셋\n",
    "- PTB corpus는 텍스트 파일로 제공되며, 원래의 PTB 문장에 전처리를 해둔 상태이다.\n",
    "- 문장이 하나의 줄로 저장되어 있다.\n",
    "- 각 문장을 연결한 하나의 큰 시계열 데이터로 취급할 수 있다, 이때 각 문장 끝에는 <eos>라는 특수 문자를 삽입한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말뭉치 크기: 929589\n",
      "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "\n",
      "word_to_id['car']: 3856\n",
      "word_to_id['happy']: 4428\n",
      "word_to_id['lexus']: 7426\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.append('../data_set')\n",
    "sys.path.append('..')\n",
    "\n",
    "from data_set import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train') # ptb를 읽어서 corpus, word_to_id, id_to_word 반환 함수\n",
    "\n",
    "print(f\"말뭉치 크기: {len(corpus)}\")\n",
    "print(f\"corpus[:30]: {corpus[:30]}\")\n",
    "print()\n",
    "print(f\"id_to_word[0]: {id_to_word[0]}\")\n",
    "print(f\"id_to_word[1]: {id_to_word[1]}\")\n",
    "print(f\"id_to_word[2]: {id_to_word[2]}\")\n",
    "print()\n",
    "print(f\"word_to_id['car']: {word_to_id['car']}\")\n",
    "print(f\"word_to_id['happy']: {word_to_id['happy']}\")\n",
    "print(f\"word_to_id['lexus']: {word_to_id['lexus']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.5 PTB 데이터셋 평가\n",
    "- 큰행렬에 SVD를 적용해야 함으로 고속 SVD를 적용한다.\n",
    "- sklearn 모듈을 설치하여 SVD를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cbigo\\AppData\\Local\\Temp\\ipykernel_20724\\2148773683.py:14: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  pmi = np.log2(C[i,j] * N / (S[j]*S[i]) + 1e-8)\n",
      "C:\\Users\\cbigo\\AppData\\Local\\Temp\\ipykernel_20724\\2148773683.py:14: RuntimeWarning: invalid value encountered in log2\n",
      "  pmi = np.log2(C[i,j] * N / (S[j]*S[i]) + 1e-8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00% 완료\n",
      "2.00% 완료\n",
      "3.00% 완료\n",
      "4.00% 완료\n",
      "5.00% 완료\n",
      "6.00% 완료\n",
      "7.00% 완료\n",
      "8.00% 완료\n",
      "9.00% 완료\n",
      "10.00% 완료\n",
      "11.00% 완료\n",
      "12.00% 완료\n",
      "13.00% 완료\n",
      "14.00% 완료\n",
      "15.00% 완료\n",
      "16.00% 완료\n",
      "17.00% 완료\n",
      "18.00% 완료\n",
      "19.00% 완료\n",
      "20.00% 완료\n",
      "21.00% 완료\n",
      "22.00% 완료\n",
      "23.00% 완료\n",
      "24.00% 완료\n",
      "25.00% 완료\n",
      "26.00% 완료\n",
      "27.00% 완료\n",
      "28.00% 완료\n",
      "29.00% 완료\n",
      "30.00% 완료\n",
      "31.00% 완료\n",
      "32.00% 완료\n",
      "33.00% 완료\n",
      "34.00% 완료\n",
      "35.00% 완료\n",
      "36.00% 완료\n",
      "37.00% 완료\n",
      "38.00% 완료\n",
      "39.00% 완료\n",
      "40.00% 완료\n",
      "41.00% 완료\n",
      "42.00% 완료\n",
      "43.00% 완료\n",
      "44.00% 완료\n",
      "45.00% 완료\n",
      "46.00% 완료\n",
      "47.00% 완료\n",
      "48.00% 완료\n",
      "49.00% 완료\n",
      "50.00% 완료\n",
      "51.00% 완료\n",
      "52.00% 완료\n",
      "53.00% 완료\n",
      "54.00% 완료\n",
      "55.00% 완료\n",
      "56.00% 완료\n",
      "57.00% 완료\n",
      "58.00% 완료\n",
      "59.00% 완료\n",
      "60.00% 완료\n",
      "61.00% 완료\n",
      "62.00% 완료\n",
      "63.00% 완료\n",
      "64.00% 완료\n",
      "65.00% 완료\n",
      "66.00% 완료\n",
      "67.00% 완료\n",
      "68.00% 완료\n",
      "69.00% 완료\n",
      "70.00% 완료\n",
      "71.00% 완료\n",
      "72.00% 완료\n",
      "73.00% 완료\n",
      "74.00% 완료\n",
      "75.00% 완료\n",
      "76.00% 완료\n",
      "77.00% 완료\n",
      "78.00% 완료\n",
      "79.00% 완료\n",
      "80.00% 완료\n",
      "81.00% 완료\n",
      "82.00% 완료\n",
      "83.00% 완료\n",
      "84.00% 완료\n",
      "85.00% 완료\n",
      "86.00% 완료\n",
      "87.00% 완료\n",
      "88.00% 완료\n",
      "89.00% 완료\n",
      "90.00% 완료\n",
      "91.00% 완료\n",
      "92.00% 완료\n",
      "93.00% 완료\n",
      "94.00% 완료\n",
      "95.00% 완료\n",
      "96.00% 완료\n",
      "97.00% 완료\n",
      "98.00% 완료\n",
      "99.00% 완료\n",
      "100.00% 완료\n",
      "PPMI Calculate END\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from data_set import ptb\n",
    "\n",
    "window_size = 2 # 동시 발생 계산시 window 크기\n",
    "wordvec_size = 100 # 압축 차원\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# 동시발생 수 계산\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "\n",
    "# PPMI 계산\n",
    "W = ppmi(C, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[you]\n",
      "i: 0.7025728225708008\n",
      "we: 0.6673679947853088\n",
      "do: 0.6136865019798279\n",
      "anybody: 0.6042898297309875\n",
      "'d: 0.5212484002113342\n",
      "\n",
      "[year]\n",
      "earlier: 0.6588015556335449\n",
      "month: 0.6476756930351257\n",
      "quarter: 0.6124095916748047\n",
      "fiscal: 0.5512375831604004\n",
      "last: 0.5508217811584473\n",
      "\n",
      "[car]\n",
      "auto: 0.6499615907669067\n",
      "luxury: 0.5790735483169556\n",
      "corsica: 0.567619264125824\n",
      "vehicle: 0.528739333152771\n",
      "gm: 0.5130618810653687\n",
      "\n",
      "[toyota]\n",
      "motor: 0.76077800989151\n",
      "motors: 0.6462806463241577\n",
      "nissan: 0.6405166387557983\n",
      "honda: 0.6358739137649536\n",
      "mazda: 0.6281181573867798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVD 계산\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=None)\n",
    "\n",
    "word_vecs = U[:, : wordvec_size] # word vector 차원 축소\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "\n",
    "for query in querys:\n",
    "    print(f\"[{query}]\")\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5) # 각 query와 유사한 단어 5개 씩을 출력해 준다.\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 정리\n",
    "- 자연어를 대상으로 컴퓨터에게 '단어의 의미'를 이해 시키기 위해서 시소러스, 통계 기반 기법이 있다.\n",
    "- 시소러스 기반 기법은 단어들의 관련성을 사람이 수작업으로 하나씩 정의 한다.\n",
    "- 이 작업은 비용이 많이 들고 표현력에도 한계가 있다.\n",
    "- 통계 기반 기법은 말뭉치로부터 단어의 의미를 자동으로 추출하고, 그 의미를 벡터로 표현한다.\n",
    "- 단어의 동시발생 행렬을 만들고 PPMI 행렬로 변환한 다음, 안전성을 높이기 위해 SVD를 이용해 차원을 감소시켜 각 단어의 분산표현을 만들어 낸다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
