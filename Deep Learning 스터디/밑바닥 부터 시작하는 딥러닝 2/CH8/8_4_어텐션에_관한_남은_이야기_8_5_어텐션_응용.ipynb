{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 어텐션에 관한 남은 이야기\n",
    "\n",
    "\n",
    "\n",
    "#### 8.4.1 양방향 RNN\n",
    "\n",
    "<img width=\"674\" alt=\"fig 8-29\" src=\"https://github.com/choibigo/Study/assets/38881179/d77d1189-58a1-466e-b0d8-6b157c764aff\">\n",
    "\n",
    "- 그림과 같이 LSTM의 각 시각의 은닉 벡터는 hs에 모아 진다.\n",
    "- 그러나 고양이에 대응 하는 벡터는 '나', '는', '고양이' 3개 단어에 대한 정보만 인코딩되어 들어간다.\n",
    "- LSTM을 양방향으로 처리하는 방법을 양방향 LSTM으로 구현할 수있다.\n",
    "\n",
    "<img width=\"672\" alt=\"fig 8-30\" src=\"https://github.com/choibigo/Study/assets/38881179/7fb57b67-e572-4282-b7bb-3d213f46ea60\">\n",
    "\n",
    "- 양방향 LSTM에서는 지금까지의 LSTM 계층에 역방향으로 처리하는 LSTM 계층을 추가한다.\n",
    "- 이 두 LSTM 계층의 은닉 상태를 연결 시킨 벡터를 최종 은닉 상태로 처리 한다.\n",
    "- 이처럼 양방향으로 처리함으러써, 각 단어에 대응하는 은닉 상태 벡터에는 좌와 우 양쪽 방향으로부터의 정보를 집약할 수 있다.\n",
    "- LSTM은 구현하기도 쉽다, 2개의 LSTM 계층을 사용하여 각각의 계층에 주는 단어의 순서를 조정하면 된다, 이후 두 LSTM 계층의 출력을 연결 하기만 하면 된다.\n",
    "\n",
    "#### 8.4.2 Attention 계층 사용 방법\n",
    "\n",
    "<img width=\"580\" alt=\"fig 8-31\" src=\"https://github.com/choibigo/Study/assets/38881179/c7bdaba6-05eb-44a1-a930-75305ea9a976\">\n",
    "\n",
    "- Attention 계층을 LSTM과 Affine 계층 사이에 삽입했다, 그러나 장소가 반드시 이 그림과 같을 필요는 없다.\n",
    "\n",
    "<img width=\"616\" alt=\"fig 8-32\" src=\"https://github.com/choibigo/Study/assets/38881179/25c5f36a-9718-4522-bd36-ab0f2e7f3849\">\n",
    "\n",
    "- Attention 계층의 출력이 다음 시각의 LSTM 계층에 입력 되도록 연결할 수 있다.\n",
    "- 이렇게 구성하면 LSTM 계층이 맥락 벡터의 정보를 이용할 수 있다.\n",
    "- Attention 계층이 위치를 다르게 하는게 최종 정확도에 어떤 영향을 줄지는 모르지만 성능을 높일 수 있는 가능성이 있다.\n",
    "\n",
    "#### 8.4.3 seq2seq 심층화와 skip 연결\n",
    "- 어텐션을 갖춘 seq2seq에도 더 높은 표현력이 요구 된다, 이때 생각할 수 있는 것은 RNN 계층을 깊게 쌓는 방법이다\n",
    "- 층을 높게 쌓으면 표현력이 높은 모델을 만들 수 있다.\n",
    "\n",
    "<img width=\"680\" alt=\"fig 8-33\" src=\"https://github.com/choibigo/Study/assets/38881179/affc4bbd-beb3-4d0b-b0f9-a65e2f0182a6\">\n",
    "\n",
    "- 위모델은 Encoder와 Decoder로 3층 LSTM 계층을 사용하고 있다.\n",
    "- Decoder의 LSTM 계층의 은닉 상태를 Attention 계층에 입력하고, Attention 계층의 출력인 맥락 벡터를 Decoder의 여러 계층으로 전파 한다.\n",
    "- 층을 깊게 연결할 때 사용되는 기법중 하나는 skip 연결이다.\n",
    "\n",
    "<img width=\"281\" alt=\"fig 8-34\" src=\"https://github.com/choibigo/Study/assets/38881179/d3f234e8-22fd-47fc-8d34-fe61e106c64e\">\n",
    "\n",
    "- skip 연결의 접속부에서는 2개의 출력이 더해 진다.\n",
    "- 이 연산에서는 덧셈이 핵심이다, 왜냐하면 덧셈은 역전파 시 기울기를 그대로 흘려 보내기 때문이다, skip 연결의 기울기가 아무런 영향을 받지 않고 모든 계층으로 흐르기 때문이다.\n",
    "- 따라서 층이 깊어져도 기울기가 손실 되지 않고 전파되어, 결과적으로 좋은 학습을 기대할 수 있다.\n",
    "\n",
    "## 8.5 어텐션 응용\n",
    "\n",
    "#### 8.5.1 구글 신경망 기계 번역\n",
    "\n",
    "<img width=\"662\" alt=\"fig 8-35\" src=\"https://github.com/choibigo/Study/assets/38881179/267ed393-112b-40b9-946f-ea3c03c72db1\">\n",
    "\n",
    "- Encoder Decoder 와 Attention으로 구성되어 있다.\n",
    "- LSTM 계층의 다층화, 양방향 LSTM, skip connection 등 다양한 기법을 통해 성능을 향삭 시켰다.\n",
    "\n",
    "#### 8.5.2 트랜스포머\n",
    "- RNN은 이전 시각에 계산하 결과를 이용하여 순차적으로 계산한다.\n",
    "- 따라서 RNN의 시간 계산을 시간 방향으로 병렬계산하는 것은 불가능하다.\n",
    "- \"Attention is all you need\" 논문에서 제안한 Transformer는 RNN을 사용하지 않고 Attention을 통해 처리한다.\n",
    "- **self-attention**이라는 기술을 이용하는게 핵심이다.\n",
    "- 하나의 시계열 데이터를 대상으로 한 어텐션으로, '하나의 시계열 데이터 내에서' 각 원소가 다른 원소들과 어떻게 관련되는지를 살펴보자는 취지이다.\n",
    "\n",
    "<img width=\"529\" alt=\"fig 8-37\" src=\"https://github.com/choibigo/Study/assets/38881179/7ee34247-0fcf-4dc1-8ad2-6da387ce00fa\">\n",
    "\n",
    "- 왼쪽은 encoder의 hidden state들과 입력을 embedding이후 LSTM을 통과한 hiden state가 입력인 기존의 Attention 이다.\n",
    "- 오른쪽은 입력 2개 모두 Hidden state들로 각자가 어떤 관계가 있나를 파악한다.\n",
    "\n",
    "<img width=\"594\" alt=\"fig 8-38\" src=\"https://github.com/choibigo/Study/assets/38881179/2e70bbaa-eef7-4213-b15b-229298b9059b\">\n",
    "\n",
    "- 트랜스포머에서는 RNN 대신 어텐션을 사용한다.\n",
    "- Encoder, Decoder 모두에서 셀프어텐션을 사용할 수 있다.\n",
    "- Foward 계층은 Feed Forward 신경망을 나타 낸다, 은닉층이 1개이고 활성화 함수로 ReLU를 이용한 완전연결계층 신경망을 이용한다.\n",
    "- **Transformer는 추가적인 정리를 할 예정**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN -> LSTM -> Seq2seq -> Attention -> Transformer\n",
    "\n",
    "#### RNN -> LSTM\n",
    "- RNN은 이전의 출력에 대해서만 입력으로 받기 때문에 바로 이전에 데이터에 대해서 영향을 많이 받는다.\n",
    "- 이를 해결하기위해 여러 게이트를 추가해 어떤 데이터를 살리고 어떤 살릴지 결정하여 데이터 손실을 막을 수 있다.\n",
    "\n",
    "#### LSTM -> Seq2seq\n",
    "- LSTM 기반으로 언어모델을 사용한다면 입력데이터의 도메인과 동일한 도메인만 출력으로 사용할 수 있다.\n",
    "- 그러나 이러는 경우는 많지 않기 때문에 도메인을 변경할 수 있는 seq2seq를 사용한다.\n",
    "- seq2seq는 Encoder-Decoder 형태를 기반으로 한다. Encoder가 입력 데이터를 벡터로 압축한다. 이후 Decoder는 압축된 벡터를 변경하고 싶은 도메인의 형태로 출력한다.\n",
    "\n",
    "#### Seq2seq -> Attention\n",
    "- Seq2seq는 Encoder에서 Decoder로 정보를 넘겨줄때 Encoder의 마지막 LSTM의 output hidden state만 전달 된다.\n",
    "- 그러나 이러한 형태는 긴 문장이든 짧은 문장이든 고정된 길이의 벡터로 변환되기 때문에 정보의 손실이 일어날 수있다.\n",
    "- 또한 하나의 벡터만 사용하면 변환되는 데이터가 입력데이터의 어디를 집중하여 변환되는지 파악할 수 없다.\n",
    "- 그렇기 때문에 Encoder에서 Decoder로 정보를 넘겨 줄때 LSTM의 각 hidden state정보를 함께 넘겨준다.\n",
    "\n",
    "#### Attention -> Transformer\n",
    "- 입력에 대해서만 관계를 파악하는 것 뿐아니라, hidden state 내에있는 벡터들 끼리 관계를 분석하여 각원소가 다른 원소들과 어떤 관련이 있는지 파악한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
