{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 LSTM을 사용한 언어 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import TimeEmbedding, TimeLSTM, TimeAffine, TimeSoftmaxWithLoss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.random import randn as rn\n",
    "from data_set import ptb\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.utill import eval_perplexity\n",
    "\n",
    "class Rnnlm:\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype(np.float64) # 전체 단어 수 에서 압축 할 차원수로 가기 위한 Weight\n",
    "        lstm_Wx = (rn(D, 4*H)/np.sqrt(D)).astype(np.float64) # Embedding된 Vector 차원 수 => hidden state 4개(f, i, o, g)로 되기 위한 Weight\n",
    "        lstm_Wh = (rn(H, 4*H)/np.sqrt(H)).astype(np.float64) # Hidden State State => 4개의 Hidden state 가 되기 위한 Weight \n",
    "        lstm_b = np.zeros(4 * H).astype(np.float64) # W들 연산 이후 더해질 Bias\n",
    "        affine_W = (rn(H, V)) # LSTM 각 layer에서 나온 Output의 Affine 연산을 한번에 계산하기 위한 TimeAffine Layer의 Weight Hidden state 수 => 분류할 총 vocab 크기\n",
    "        affine_b = np.zeros(V).astype(np.float64)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "    \n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n",
    "    \n",
    "    def save_params(self, file_name='Rnnlm.pkl'):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dumps(self.params, f)\n",
    "    \n",
    "    def load_params(self, file_name='Rnnlm.pkl'):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            self.params = pickle.load(f)\n",
    "\n",
    "\n",
    "# ===== 학습 =====\n",
    "\n",
    "# 하이퍼파라미터\n",
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 100\n",
    "time_size = 35\n",
    "lr = 20.0\n",
    "max_epoch = 4\n",
    "max_grad = 0.25\n",
    "\n",
    "# 학습데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "tx = corpus[1:]\n",
    "\n",
    "# 모델 생성\n",
    "model = Rnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "trainer.fit(xs, tx, max_epoch, batch_size, time_size, max_grad, eval_interval=20)\n",
    "trainer.plot(ylim=(0, 500))\n",
    "\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print(f\"Perflexity Test: {ppl_test}\")\n",
    "\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 RNNLM 추가 개선\n",
    "\n",
    "- LSTM 계층 다층화\n",
    "- Dropout을 통한 Overfitting 방지\n",
    "- Weight Tying을 통한 학습 매개변수 감소 및 정확도 증가\n",
    "\n",
    "#### 6.5.1 LSTM 계층 다층화\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/d308c8dd-5f01-4aec-a103-8a965c408f70)\n",
    "\n",
    "- LSTM을 겹쳐서 사용하면 언어의 정확도가 향샹된다.\n",
    "- 1번쨰 LSTM 계층의 Hidden state가 2번째 계층의 Input으로 입력된다.\n",
    "\n",
    "#### 6.5.2 드롭아웃에 의한 과적합 억제\n",
    "- LSTM 계층을 다층화 하면 시계열 데이터의 복잡한 의존 관계를 학습할 수 있을것이라 기대한다.\n",
    "- 층을 깊게 함으로써 표현력이 풍부한 모델을 만들 수 있다.\n",
    "- 그러나 'Over fitting'문제가 발생할 수 있다.\n",
    "- RNN은 일반적인 피드포워드 신경망보다 쉽게 과적합을 일으킬 수 있다.\n",
    "- 따라서 RNN에도 Regularization, Normalize, Dropout과 같은 과적합 대책이 중요하다\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/b0f3af2b-d5b8-4a88-945b-68dcccaf2b07)\n",
    "\n",
    "- 시계열 방향으로 Dropout Layer를 삽입하는 것은 좋은 방법이 아니다.\n",
    "- 시간에 흐름에 따라 정보가 사라질 수 있다. 즉, 시간에 비례해 드롭아웃에 의한 노이즈가 축적 된다.\n",
    "- 시간에 흐름에 따른 기억 손실을 예방하려고 Cell을 만들었는데 Dropout을 적용해 버리면 소용없어 진다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/3cc04141-7a33-4578-96bc-dc8cb5a105b0)\n",
    "\n",
    "- Dropout Layer를 깊이 방향 으로 삽입할 수 도 있다.\n",
    "- 이렇게 구성하면 시간 방향으로 아무리 진행해도 정보를 잃지 않는다.\n",
    "- 드롭아웃이 시간축과 독립적으로 깊이 방향으로만 영향을 준다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/ed716177-6a17-43f1-a796-118f8b659589)\n",
    "\n",
    "- 일반저깅 Dropout은 시간 방향에는 적합하지 않다.\n",
    "- 그러나 최근 연구에서는 RNN의 시간 방향 정규화를 목표로하는 다양한 방법이 제안된다.\n",
    "- Variational Dropout을 통해 시간 방향으로 Dropout을 적용하는데 성공했다.\n",
    "- Variational Dropout은 깊이 방향과 시간 방향에도 이용할 수 있어 언어 모델의 정확도를 향상시킬 수 있다.\n",
    "- 같은 계층에 속한 Dropout들은 같은 Mask를 공유한다, Mask는 데이터의 통과/차단을 결정하는 Binary 형태의 무작위 패턴이다.\n",
    "- 위 그림에서 색이 같은 Dropout 계층 들은 고정된 Mask를 이용한다.\n",
    "- 마스크를 고정함으로써 정보를 잃게 되는 방법도 '고정'되므로, 일반적인 드롭아웃 때와 달리 정보가 지수적으로 손실되는 것을 방지할 수 있다.\n",
    "\n",
    "#### 6.5.3 가중치 공유\n",
    "- 언어 모델을 개선하는 간단한 트릭은 'Weight tying이 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/5cff8d01-a854-42d6-901a-4dfbfa367062)\n",
    "\n",
    "- Embedding 계층의 가중치와 Affine 계층의 가중치를 연결하는 기법이 가중치 공유(Weight Tying)이다.\n",
    "- 가중치를 공유함으로써 매개변수 수를 줄일 수 있다 이에 따라 학습이 쉬워져 정확도도 향상되는 기법이다.\n",
    "- 어휘수를 V Hidden sTate의 수를 H라고 했을 때, Embedding 계층의 가중치는 V x H로 나타낼 수 있다, Affine 계층을 H x v 이다.\n",
    "- Emgedding 계층의 가중치를 전치하여 Affine 계층의 가중치로 설정할 수 있다.\n",
    "\n",
    "#### 6.5.4 개선된 RNNLM 구현\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/9405f74e-5c2a-4172-a2c0-fe7ea274beac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "import numpy as np\n",
    "from common.base_model import BaseModel\n",
    "from numpy.random import randn as rn\n",
    "\n",
    "\n",
    "class BetterRnnlm(BaseModel):\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=650, hidden_size=650, dropout_ratio=0.5):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        embed_W = (rn(V, D) / 100).astype(np.float64)\n",
    "        lstm_Wx1 = (rn(D, 4*H) / np.sqrt(D)).astype(np.float64)\n",
    "        lstm_Wh1 = (rn(H, 4*H) / np.sqrt(H)).astype(np.float64)\n",
    "        lstm_b1 = np.zeros(4*H).astype(np.float64)\n",
    "        lstm_Wx2 = (rn(D, 4*H) / np.sqrt(D)).astype(np.float64)\n",
    "        lstm_Wh2 = (rn(H, 4*H) / np.sqrt(H)).astype(np.float64)\n",
    "        lstm_b2 = np.zeros(4*H).astype(np.float64)\n",
    "        affine_b = np.zeros(V).astype(np.float64)\n",
    "\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeAffine(embed_W.T, affine_b)\n",
    "        ] \n",
    "        # dropout을 각 layer 마다 추가함\n",
    "        # Affine과 Embedding Weight를 공유함\n",
    "        # LSTM계층을 깊게 쌓음\n",
    "\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layers = [self.layers[2], self.layers[4]]\n",
    "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
    "        \n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs, train_flg=False):\n",
    "        for layer in self.drop_layers:\n",
    "            layer.train_flg = train_flg\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts, train_flg=True):\n",
    "        score = self.predict(xs, train_flg)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def reset_state(self):\n",
    "        for layer in self.lstm_layers:\n",
    "            layer.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.utill import eval_perplexity\n",
    "from data_set import ptb\n",
    "\n",
    "#Hyper Parameter\n",
    "batch_size = 20\n",
    "wordvec_size = 650\n",
    "hidden_size = 650\n",
    "time_size = 35\n",
    "lr = 20.0\n",
    "max_epoch = 40\n",
    "max_grad = 0.25\n",
    "dropout_ratio = 0.5\n",
    "\n",
    "# Data Load\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_val, _, _ = ptb.load_data('val')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "tx = corpus[1:]\n",
    "\n",
    "model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout_ratio)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "# Early Stop & Scheduling\n",
    "best_ppl = float('inf')\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(xs, tx, max_epoch=1, batch_size=batch_size, time_size=time_size, max_grad=max_grad)\n",
    "\n",
    "    model.reset_state()\n",
    "    ppl = eval_perplexity(model, corpus_val)\n",
    "    print(f\"Perplexity : {ppl}\")\n",
    "\n",
    "    if best_ppl > ppl:\n",
    "        best_ppl = ppl\n",
    "        model.save_params()\n",
    "    else:\n",
    "        lr /= 4.0\n",
    "        optimizer.lr = lr\n",
    "    \n",
    "    model.reset_state()\n",
    "    print('='*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
