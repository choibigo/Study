{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 LSTM 구현\n",
    "\n",
    "- 한 단계만 처리하는 LSTM 클래스를 구현한 다음, 이 어서 T개의 단계를 한꺼번에 처리하면 LSTM을 구현한다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/7c78c190-7bfd-4f40-a9a6-438a6bd01e63)\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/7aa8b5ff-8f8a-42a1-b2b4-ad67bce69b45)\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/e6307081-20a9-411e-916d-240af846902e)\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/5af03eed-dc2c-4039-9c11-a96fbc5549a9)\n",
    "\n",
    "- f : forget gate로 입력된 cell을 얼마나 '잊을'건지를 결정하는 연산이다, 이 결과가 입력 셀과 곱해져 입력 셀의 잊는 정도를 결정한다.\n",
    "- g : Memory cell로 입력 데이터를 기억하기 위해 사용하는 연산이다.\n",
    "- i : input gate로 입력 데이터에 대해 얼마나 기억할 지를 결정하는 연산이다, g 연산과 곱해져 입력데이터를 얼마나 기억할지 정도를 결정한다.\n",
    "- o : output gate로 연산된 cell을 얼마나출력 할지 결정하는 연산이다, 이 cell과 곱해져 출력 셀을 얼마나 내보낼지 결정한다.\n",
    "- ct(다음 출력할 cell) : forget gate와 이전 cell 의 원소별 곱 과 Memory cell과 Input Gate가 더해지는 연산이다.\n",
    "- ht(hidden state): cell tanh가 곱해진 결과와 Output gate가 곱해지는 연산이다.\n",
    "\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/103277a1-c219-4b50-80d4-6684d3419784)\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/010b4d62-0dc8-4760-a8e6-c65dd619b1da)\n",
    "\n",
    "- f, g, i, o 연산이 각각 수행되지만 모두 Affine 연산(곱이 더해진 형태)이다, 이를 행렬 곱으로 계산할 수 있다.\n",
    "- 이를 통해 연산의 속도를 높일 수 있다.\n",
    "- 각 연산이 필요한 Wx와 Wh를 하나의 행렬로 만들어 큰 Wx, Wh를 만든다.\n",
    "- 연산이 끝난 후 Slice 연산을 통해 각 Affine 결과를 얻은 후, 각각의 활성화 함수를 거쳐서 최종적이 f, g, i, o를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.function import sigmoid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        # 초기 가중치 로서 앞서 말한 4개 연산에 대한 가중치가 담겨 있다\n",
    "\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        # 입력과 이전의 c, h를 입력으로 받는다.\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape # 입력 데이터의 미니배치수 x hidden state 차원수\n",
    "        A = np.matmul(x, Wx) + np.matmul(h_prev, Wh) + b # 행렬로 Affine 연산\n",
    "\n",
    "        #slice\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:H*2]\n",
    "        i = A[:, H*2:H*3]\n",
    "        o = A[:, H*3:]\n",
    "\n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i # 출력할 ct의 최종 연산\n",
    "        h_next = o * np.tanh(c_next) # output gate와 c_next를 곱한 결과\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "    \n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "        \n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "        \n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "        \n",
    "        dc_prev = ds * f\n",
    "        \n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "        \n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "        \n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "        \n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "        \n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "        \n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "        \n",
    "        return dx, dh_prev, dc_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://github.com/choibigo/Study/assets/38881179/d2878136-5b1e-4a18-a4f2-fe34feb7698d)\n",
    "\n",
    "- N : 미니 배치 수\n",
    "- D : 입력 데이터의 차원 수\n",
    "- H : 은닉 상태의 차원 수\n",
    "- f, g, i, o를 하나의 weight 행렬로 묶었기 때문에 LSTM 계층에서도 3개의 매개변수(Wx, Wh, b)만 관리하면 된다.\n",
    "- 그러나 RNN과 각 Weight의 형상이 다르다.\n",
    "\n",
    "#### 역전파\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/acb92725-1f07-4913-aa82-271cbbcce0a7)\n",
    "\n",
    "- Slice는 나눠서 분배하는 연산이다, 반대로 역전파는 4개의 기울기를 결합 해야한다.\n",
    "- DA의 연산은 각 기울기를 하나로 묶는 연산 이며 Numpy의 hstack() 메소드를 활용할 수 있다, 이 를 이용하여 배열을 가로로 연결할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1 Time LSTM 구현\n",
    "- TimeLSTM은 T개 분의 시계열 데이터를 한꺼번에 처리하는 계층이다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/3a1368e2-1a2b-4de8-b774-fbcf71975de9)\n",
    "\n",
    "- Truncated BPTT는 역전파의 연결은 적당히 끊고 순전파의 흐름은 그대로 유지한다.\n",
    "- 은닉 상태와 기억 셀을 인스턴스 변수로 유지하도록 한다.\n",
    "- 이렇게 하면 forward()를 호출했을 때 이전 시각의 은닉 상태, 기억셀에서 부터 시작할 수 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/99031104-59e3-4d2b-a007-a553040537c6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        self.h, self.c = None, None # Truncated BPTT를 사용하기 위해 이용하는 h와 c를 위한 Instance 변수\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        # 적당한 크기의 시계열 데이터(T)가 주어진다.\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype=np.float64) # 배치사이즈 x Truncated BPTT를 수행시 주어지는 데이터 개수 X hidden state 차원수\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            # 1) state를 사용하지 않는 경우\n",
    "            # 2) h가 없는 경우\n",
    "            # h를 초기화 한다.\n",
    "            self.h = np.zeros((N, H), dtype=np.float64)\n",
    "\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype=np.float64)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params) # LSTM을 T번 반복 한다.\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c) # t번째 데이터를 LSTM 1개에 통과 시킨후 hidden state와 cell을 얻는다.\n",
    "            hs[:, t, :] = self.h # hs Update\n",
    "\n",
    "            self.layers.append(layer) # backward 하기위해 Layer 정보 저장\n",
    "        \n",
    "        return hs # TimeLSTM의 최종 output은 전체 hs이다.\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype=np.float64)\n",
    "        dh, dc = 0,0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t] # t번째 Layer\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "            \n",
    "            for i, grad in enumerate(grads):\n",
    "                self.grads[i][...] = grad\n",
    "\n",
    "        self.dh = dh\n",
    "        return dxs # 최종 backpropa\n",
    "    \n",
    "    def set_state(self, h, c=None):\n",
    "        self.h = h\n",
    "        self.c = c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        self.c = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
