{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 언어 모델을 사용한 문장 생성\n",
    "\n",
    "### 7.1.1 RNN을 사용한 문장 생성의 순서\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/097c329a-0b7c-416e-af2f-65e2a387a526)\n",
    "\n",
    "- 시계열 데이터 T개분을 모아 처리하는 TimeLSTM 과 Affine 계층으로 구성\n",
    "- 만약 입력으로 \"I\"가 주어지면 1개의 LSTM 계층에서 다음에 출현하는 단어의 확률분포를 출력한다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/ea536885-b3e0-4f3a-8c56-06570c0f9f45)\n",
    "\n",
    "- 단어를 새로 생성하는 방법을 살펴본다.\n",
    "- 첫 번째, 확률이 가장 높은 던어를 선택하는 방법을 떠올릴 수 있다, 확률이 가장 높은 단어를 선택할 뿐이므로 결과가 일정하게 정해지는 '결정적'인 방법이다.\n",
    "- 또한 '확률적'으로 선택하는 방법도 있다, 각 후보 단어의 확률(Softmax 통과 확률)에 맞게 선택하는 것으로, 확률이 높은 단어가 선택되기 쉽고, 확률이 낮은 단어는 선택되기 어렵다.\n",
    "- 이 방식은 선택되는 단어(샘플링 단어)가 매번 다를 수 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/b6a4f306-f859-44e5-8e9e-71ef7d813fc6)\n",
    "\n",
    "- 확률 분포로 부터 샘플링을 수행하여 'say'가 선택될 수 있다.\n",
    "- 'say'의 확률이 가장 높아서가 아닌 확률 분포에서 샘플링한 결과인것을 주의 하자\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/5ad5e962-50b9-4e4c-834b-d9bb15d82a08)\n",
    "\n",
    "- 샘플링을 반복하여 ```<eos>```와 같이 종결 기호가 나올때 까지 반복한다.\n",
    "- 그러면 새로운 문장을 생성할 수 있다.\n",
    "- 여기서 주목할 것은 이렇게 생성한 문장은 훈련데이터에는 존재하지 않는 문장이다, 따라서 말 그대로 새롭게 생성된 문장이다.\n",
    "- 모델이 새로 훈련 데이터에서 사용된 단어의 ```정렬 패턴```을 학습한 것이므로 새로 생성한 문장도 자연스럽고 의미가 통하는 문장일 것이라 기대한다.\n",
    "\n",
    "#### 7.1.2 문장 생성 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import TimeEmbedding, TimeLSTM, TimeAffine, TimeSoftmaxWithLoss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.random import randn as rn\n",
    "from data_set import ptb\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.utill import eval_perplexity\n",
    "\n",
    "class Rnnlm:\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype(np.float64) # 전체 단어 수 에서 압축 할 차원수로 가기 위한 Weight\n",
    "        lstm_Wx = (rn(D, 4*H)/np.sqrt(D)).astype(np.float64) # Embedding된 Vector 차원 수 => hidden state 4개(f, i, o, g)로 되기 위한 Weight\n",
    "        lstm_Wh = (rn(H, 4*H)/np.sqrt(H)).astype(np.float64) # Hidden State State => 4개의 Hidden state 가 되기 위한 Weight \n",
    "        lstm_b = np.zeros(4 * H).astype(np.float64) # W들 연산 이후 더해질 Bias\n",
    "        affine_W = (rn(H, V)) # LSTM 각 layer에서 나온 Output의 Affine 연산을 한번에 계산하기 위한 TimeAffine Layer의 Weight Hidden state 수 => 분류할 총 vocab 크기\n",
    "        affine_b = np.zeros(V).astype(np.float64)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "    \n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n",
    "    \n",
    "    def save_params(self, file_name='Rnnlm.pkl'):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dumps(self.params, f)\n",
    "    \n",
    "    def load_params(self, file_name='Rnnlm.pkl'):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            self.params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "import numpy as np\n",
    "from common.base_model import BaseModel\n",
    "from numpy.random import randn as rn\n",
    "\n",
    "\n",
    "class BetterRnnlm(BaseModel):\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=650, hidden_size=650, dropout_ratio=0.5):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        embed_W = (rn(V, D) / 100).astype(np.float64)\n",
    "        lstm_Wx1 = (rn(D, 4*H) / np.sqrt(D)).astype(np.float64)\n",
    "        lstm_Wh1 = (rn(H, 4*H) / np.sqrt(H)).astype(np.float64)\n",
    "        lstm_b1 = np.zeros(4*H).astype(np.float64)\n",
    "        lstm_Wx2 = (rn(D, 4*H) / np.sqrt(D)).astype(np.float64)\n",
    "        lstm_Wh2 = (rn(H, 4*H) / np.sqrt(H)).astype(np.float64)\n",
    "        lstm_b2 = np.zeros(4*H).astype(np.float64)\n",
    "        affine_b = np.zeros(V).astype(np.float64)\n",
    "\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeAffine(embed_W.T, affine_b)\n",
    "        ] \n",
    "        # dropout을 각 layer 마다 추가함\n",
    "        # Affine과 Embedding Weight를 공유함\n",
    "        # LSTM계층을 깊게 쌓음\n",
    "\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layers = [self.layers[2], self.layers[4]]\n",
    "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
    "        \n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs, train_flg=False):\n",
    "        for layer in self.drop_layers:\n",
    "            layer.train_flg = train_flg\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts, train_flg=True):\n",
    "        score = self.predict(xs, train_flg)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def reset_state(self):\n",
    "        for layer in self.lstm_layers:\n",
    "            layer.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.function import softmax\n",
    "\n",
    "class RnnlmGen(BetterRnnlm):\n",
    "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
    "        # start_id : 최초로 주어지는 단어 ID\n",
    "        # sample_size : 샘플링 하는 단어수 (만드는 문장의 단어수)\n",
    "        # skip_ids : 단어 ID 리스트로서 이 리스트에 속하는 단어는 샘플링 되지 않도록 한다, <unk>나 N 등 전처리된 단어를 샘플링 하지 않게 하는 용도로 사용한다.\n",
    "\n",
    "        word_ids = [start_id]\n",
    "\n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1, 1) # predict 구현이 배치로 구현되어 있기 때문에 1개만 처리 하더라도 1x1로 형태를 변경한다.\n",
    "            score = self.predict(x) # 예측된 단어의 Score를 호출 한다.\n",
    "            p = softmax(score.flatten()) # 출력된 Score를 softmax를 통해서 확률로 변환한다.\n",
    "\n",
    "            sampled = np.random.choice(len(p), size=1, p=p) # 확률분포 p로 부터 데이터를 샘플링한다.\n",
    "            if (skip_ids is None) or (sampled not in skip_ids): # skip_ids에 sampling된 id가 있다면 건너 뛴다.\n",
    "                x = sampled # 출력을 입력으로 변경한다.\n",
    "                word_ids.append(int(x))\n",
    "\n",
    "        return word_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you think but paper difficult into that.\n",
      " with women\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from data_set import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "model = RnnlmGen()\n",
    "# model.load_params('./Rnnlm.pkl')\n",
    "model.load_params('./BetterRnnlm.pkl')\n",
    "\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "\n",
    "# Generate Sentence\n",
    "word_ids = model.generate(start_id, skip_ids, sample_size=10)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
