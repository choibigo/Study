{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "- 지금까지 살펴본 신경망은 Feed Forward 유형의 신경망이다.\n",
    "- Feed Forward는 단방향 신경망을 말한다, 다시 말해, 입력 신고가 다음 층(중간 층)으로 전달되고, 그 신호를 받은 층은 그다음 층으로 전달하고 하는 식으로 한 방향으로만 신호가 전달 된다.\n",
    "- 피드포워드 신경망은 구성이 단순하여 구조를 이해하기 쉽고, 그래서 많은 문제에 응용할 수 있다.\n",
    "- 피드포워드 신경망의 문제점은 구성이 단순하여 구조를 이해하기 쉽고, 많은 문제에 응용할수 있다.\n",
    "- 그러나 시계열 데이터를 잘 다루지 못하는 문제점이 있다.\n",
    "- 피드포워드 신경망에서는 시계열 데이터의 성질을 충분히 학습할 수 없다, 이때문에 RNN이 등장하게 됬다.\n",
    "\n",
    "## 5.1 확률과 언어 모델\n",
    "\n",
    "#### 5.1.1 word2vec을 확률 관점으로 바라보다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/d31d353f-8149-438c-b85a-733e578d3e9a)\n",
    "\n",
    "- 그림과 같이 있을떄 wt가 타겟이 될 확률은 P(Wt | Wt-1, Wt+1)이다.\n",
    "- Wt-1, Wt+1 (맥락)이 주어졌을떄, Wt가 일어날 확률을 의미한다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/e65b4ec4-f18f-42d0-a8f6-c53315b98092)\n",
    "\n",
    "- 윈도우를 한쪽으로만 고려할 수 있다.\n",
    "- 이떄 P(Wt|Wt-2, Wt-1)이라 할 수 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/0fb2757e-4842-4124-bd30-3a95aa3d428b)\n",
    "\n",
    "\n",
    "- 이를 corss entropy로 유도하면 위와 같이 나타낼 수 있다.\n",
    "- label이 있는 classificaion 문제라면 앞에 label이 곱해져야 하나, P(Wt)자체가 있다면 1 없다면 0이 되기 때문에 따로 곱해줄 필요가 없다.\n",
    "- CBOW 모델을 위 Loss 가 최소화 되도록 학습할 수 있다.\n",
    "- 이러한 매개변수가 발견되면 CBOW 모델은 맥락으로부터 타깃을 정확하게 추측할 수 있다.\n",
    "- CBOW의 본래 목적은 맥락으로부터 타깃을 정확하게 추측하는 것이다 이를 통해 단어의 의미가 인코딩된 '단어의 분산 표현'을 얻을 수 있다.\n",
    "- 그렇다면 단어의 분산 표현이 아닌 '맥락으로부터 타깃을 추측하는 것'에 주목해 실용적인 쓰임이 있을까? 이때 언어 모델이 등장한다.\n",
    "\n",
    "#### 5.1.2 언어 모델\n",
    "- 언어 모델은 단어 나열에 확률을 부여한다.\n",
    "- 특정한 단어의 시퀀스에 대해서, 그 시퀀스가 일어날 가능성이 어느 정도 인지를 확률로 평가하는 것이다.\n",
    "- 예를 들어 \"you say goodbye\"라는 단어 시퀀스에는 높은 확률을 출력하고 \"you say good die\"에는 낮은 확률을 출력하는 것이 일종의 언어 모델이다.\n",
    "- 언어 모델의 대표적인 예는 기계 번역과 음석인식이다.\n",
    "- 음성 인식 시스템의 경우, 사람의 음성으로 부터 몇 개의 문장을 후보로 생성할 것이다, 그런다음 언어 모델을 사용하여 후보 문장이 '문장으로써 자연스러운지'를 기준으로 순서를 매길 수 있다.\n",
    "- 언어 모델은 새로운 문장을 생성하는 용도로도 이용할 수 있다.\n",
    "- 왜냐하면 언어 모델은 단어 순서의 자연스러움을 확률적으로 평가할 수 있으므로, 그 확률분포에 따라 다음으로 적합한 단어를 샘플링할 수 있기 때문이다.\n",
    "- W1 ~ Wm 까지 m개 단어로 된 문장을 생각할 수 있다, 이 단어들이 순서대로 등장할 확률을 P(W1, ... Wm)으로 나타낸다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/c7d46252-0d77-4bc6-beec-25438a2e5719)\n",
    "\n",
    "- 동시 확률은 사후확률을 사용하여 다음과 같이 분해할 수 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/be4bfd8b-7a10-4270-80eb-d3370516824c)\n",
    "\n",
    "- 곱셈 정리를 이용하여 나타낼 수 있다.\n",
    "- 곱셈정리는 확률론에서 가장 중요한 정리이다.\n",
    "- A와 B가 모두 일어날 확률은 P(A, B)는 B가 일어날 확률은 P(B)와 B가 일어난 후 A가 일어날 확률 P(A|B)를 곱한 값과 같다는 것이다.\n",
    "- 곱셈 정리를 이용하여 P(W1, ... Wm)을 사후 확률로 나타낼 수 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/ca6c03e2-9bf4-4191-aa8e-c5aeeb68f810)\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/c8e5cbf6-1c33-4c02-b4c9-1306d0a475c1)\n",
    "\n",
    "- 동시 확률을 하나씩 쪼객가며 파이 식(맨위의 식)을 유도할 수 있다.\n",
    "- 즉 동시 확률을 사후 확률의 총 곱으로 대표할 수 있다.\n",
    "- 이 사후 확률은 타깃 단어보다 왼쪽에 있는 모든 단어를 맥락(조건)으로 했을 떄의 확률이다. (윈도우가 왼쪽으로만 있을때 = 이전에 나온 단어들을 아는 경우)\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/b782fcf2-6c03-4897-bf6c-4b4d25723c86)\n",
    "\n",
    "- 사후 확률(P(Wt|w1, w2 ... wt-1))을 얻을 수 있다면 곱을 통해 동시 확률(P(w1, ... wm))을 구할 수 있다.\n",
    "\n",
    "#### 5.1.3 CBOW 모델을 언어 모델로?\n",
    "- word2vec의 CBOW 모델을 억지로 언어 모델에 적용할 수도 있다.\n",
    "- 맥락의 크기를 특정값으로 한정하여 나타낼 수 있다.\n",
    "- 맥락의 크기는 임의로 설정할 수 있지 만 결국 특정 길이로 '고정'된다 예를 들어 왼쪽 10개의 단어를 맥락으로 하는 CBOW를 만들어도 그 맥락보다 더 왼쪽에 있는 단어들으 무시 된다. => 이전에 나온 모든 단어를 고려 할 수 없다.\n",
    "- window size를 엄청 키우면? => 키우면 모든 문장의 단어를 고려할 수 있겟지만 그 순서가 무시 된다.\n",
    "- 왜냐하면 CBOW 모델에서는 2개 이상의 단어에 대해 단어 벡터의 단순 'concat'이 은닉층의 입력이 되기 때문이다.\n",
    "- 언어 모델을 위해서는 맥락의 순서도 고려를 해줘야 한다.\n",
    "- 순환 신경망은 맥락이 아무리 길더라도 그 맥락의 정보를 기억하는 메커니즘을 갖추고 있다.\n",
    "- 그래서 RNN을 사용하면 아무리 긴 시계열 데이터라도 대응할 수 있다.\n",
    "\n",
    "\n",
    "```\n",
    "- CBOW는 단어의 분산 표현을 얻을 목적으로 고안된 기법이다.\n",
    "- 따라서 이를 그대로 언어 모델에 적용하는 경우는 없다.\n",
    "- RNN과 상반된 예를 들기 위해 CBOW를 언어 모델의 예시로 든것이다.\n",
    "```\n",
    "\n",
    "## 5.2 RNN 이란\n",
    "- Recurrent Neural Network 로 Recurrent는 몇번 이나 반복해서 일어나는 일 을 뜻한다.\n",
    "- 직역 하면 순환 신경망이 된다.\n",
    "\n",
    "#### 5.2.1 순환하는 신경망\n",
    "- 순환 하기 위해서는 '닫힌 경로'가 필요하다.\n",
    "- '닫힌 경로'가 존재 해야 데이터가 같은 장소를 반복해 순환할 수 있다, 또한 순환 하면서 정보가 끊김없이 생신되게 된다.\n",
    "- RNN의 특징은 순환 하는 경로(닫힌 경로)가 있다는 것이다.\n",
    "- 데이터가 순환하기 때문에 과거의 정보를 기억하는 동시에 최신 데이터로 갱신될 수 있다는 것이다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/e9808080-19d3-4102-9438-d8c24d1a4f13)\n",
    "\n",
    "- 순환 경로를 따라 데이터를 계층 안에서 순환 시킬 수 있다.\n",
    "- Xt를 입력받는데, t는 시간을 의미한다. 시계열 데이터(x0, x1, ... xt)가 RNN 계층에 입력됨을 표현한 것이다.\n",
    "- 그리고 그 입력에 대응하여 (h0, h1, ... ht )가 출력 된다.\n",
    "- 문장을 다루는 경우를 예로 든다면 각 단어의 분산 표현(단어 벡터)가 Xt가 된다. \n",
    "\n",
    "#### 5.2.2 순환 구조 펼치기 \n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/f36198dc-7918-4cfa-8452-85e6e863d70d)\n",
    "\n",
    "- RNN 계층의 순환 구조를 펼침으로써 오른쪽으로 긴 신경망이 된다.\n",
    "- 지금 까지 본 피드 포워드 구조의 네트워크와는 다르다.\n",
    "- 각 RNN 계층은 그 계층으로의 입력과 1개 전의 RNN 계층으로부터의 출력을 받습니다.\n",
    "- 이 두 정보를 바탕으로 현 시각의 출력을 계산한다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/a8bc484e-2738-4029-a0b0-ea9cd7787efe)\n",
    "\n",
    "- RNN에는 2개의 가중치가 있다.\n",
    "- 하나는 입력 x를 출력 h로 변환하기 위한 가중치 Wx이고, 다른 하나는 1개의 RNN 출력을 다음 시각의 출력으로 변환 하기 위한 가중치 Wh 이다.\n",
    "- Wx는 정확한 출력을 내도록 학습되고, Wh는 이전의 입력을 얼만큼 반영할까에 대한 정보? 일듯\n",
    "- tanh를 통과한 결과가 최종 출력이 된다, ht는 최종 결과 임과 동시에 ht+1의 입력으로 도 사용된다.\n",
    "- ht는 ht-1을 기초해 계산됨을 알 수 있다 다른 관점에서 보면 이전의 상태를 가진 상태에서 다음 상태로 계산되는 것을 알 수 있다.\n",
    "- 그래서 RNN 계층을 '상태를 가지는 계층' 혹은 '메모리(기억력)가 있는 계층' 이라고 한다.\n",
    "\n",
    "#### 5.2.3 BPTT\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/308f357f-595d-45f8-9464-64ea3cc309f4)\n",
    "\n",
    "- RNN에는 일반적인 오차역전파법을 적용할 수 있다.\n",
    "- 순전파를 수행하고 이어서 역전파를 수행하여 원하는 기울기를 구할 수 있다.\n",
    "- 여기서의 오차역전파법은 '시간 방향으로 펼친 신경망의 오차역전파법'이라는 뜻으로 BPTT라고 한다.\n",
    "- BPTT를 이용하면 RNN을 학습할 수 있어 보인다.\n",
    "- 그러나 길이간 긴 시계열 데이터를 사용할 때문제가 될 수 있다, 시계열 데이터의 시간 크기가 커지는 것에 비례하여 BPTT가 소비하는 컴퓨팅 자원도 증가하기 때문이다.\n",
    "- 또한 길이가 길어짐에 따라 신경망을 하나 통과할 때마다 기울기 값이 조금씩 작아져서, 이전 시각 까지 역전파 되기 전에 gradient가 0이되어 학습이 되지 않을 수 도 있다.\n",
    "\n",
    "#### 5.2.4 Truncated BPTT\n",
    "- 큰 시계열 데이터를 처리할 때는 흔히 신경망 연결을 적당한 길이로 끊는다.\n",
    "- 시간축 방향으로 너무 길어진 신경망을 적당한 지점에서 잘라내 작은 신경망으로 맏느는 아이디어이다.\n",
    "- 순전파는 그대로 유지하고 오차역전파만 끊어야 한다.\n",
    "- 연전파의 연결을 잘라버리면 그 보다 미래의 데이터에 대해서는 생각할 필요가 없어진다. (중간에 끊긴 곳에서 역전파 하면 되기 때문에 미래의 데이터는 신경쓰지 않는다.)\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/7928ac2b-bf88-4c69-a3cf-b0e1dce26eaa)\n",
    "\n",
    "- 순전파는 그대로 끝까지 가면된다.\n",
    "- 역전파는 앞선 시각으로부터의 기울기는 끊겼기 때문에 끊겨진 블록 안에서만 오차역전파를 계산하면된다.\n",
    "- 순전파의 연결을 유지하면서 블록단위로 오차 역전파법을 적용할 수 있다.\n",
    "- 첫 블록에대해 순전파 이후 역전파를 계산, 두 번째 블록에서 첫번쨰 블록의 출력을 입력으로 받고 블록 내에서만 역전파를 수행한다.\n",
    "\n",
    "#### 5.2.5 Truncated BPTT의 미니 배치 학습\n",
    "- 미니배치 학습을 수행할 떄는 각 미니배치의 시작 위치를 오프셋으로 옮겨준 후 순서대로 제공하면된다.\n",
    "- 데이터를 순서대로 입력하다가 끝에 도달하면 다시 처음부터 입력하도록 한다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/12951bff-5355-42e5-afc3-3cabdf088249)\n",
    "\n",
    "- 첫 번째 미니 배치때는 처음부터 순서대로 데이터를 제공\n",
    "- 두 번쨰 미니 배치 때는 500번쨰(off set) 데이터를 시작 위치로 정하고 그 위치부터 다시 순서대로 데이터를 제공\n",
    "- 데이터를 순서대로 제공하기, 미니배치별로 데이터를 제공하는 시작 위치 옮기기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
