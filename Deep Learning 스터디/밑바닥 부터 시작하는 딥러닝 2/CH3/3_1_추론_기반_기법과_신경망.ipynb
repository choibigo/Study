{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "- 앞장에서는 '통계 기반 기법'으로 단어의 분산 표현을 얻었다.\n",
    "- 이번 장에서는 더 강력한 기법인 '추론 기반 기법'을 살펴본다.\n",
    "- word2vec은  단어를 벡터로 구현하는 방법 중 하나이다.\n",
    "\n",
    "## 추론 기반 기법과 신경망\n",
    "- 단어를 벡터로 표현하는 방법은 '통계 기반 기법'과 '추론 기반 기법' 두가지가 있다.\n",
    "- 단어의 의미를 얻는 방식은 다르지만 그 배경에는 모두 분포 가설이 있다.\n",
    "\n",
    "#### 3.1.1 통계 기반 기법의 문제점\n",
    "- 통계 기반 기법은 동시 발생 행렬을 만들고 그 행렬에 SVD를 적용하여 밀집 벡터를 얻어 차원을 축소 하는 방법을 이용했다.\n",
    "- 그러나 이방식은 대규모 corpus를 다룰때 문제가 있다, 실제 데이터 100만 차원이라면 100만x100만 크기를 SVD를 적용하여 차원축소 해야한다.\n",
    "- 동시 발생 행렬과 PPMI, SVD를 이용해 전체 데이터에 대해 단 1회 만에 단어의 분산 표현을 구한다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/66f57d05-1472-4006-9b8c-b108a18dde5e)\n",
    "\n",
    "- 추론 기반 기법에서는 한번에 소량의 학습 샘플씩을 반복하며 가중치를 갱신해 간다.\n",
    "- 어떠한 corpus는 어휘 수가 많아 SVD 등 계산량이 큰 작업을 처리하기 어려운 경우에도 신경망을 학습 하여 단어의 분산 표현을 구할 수 있다는 것이다.\n",
    "\n",
    "#### 3.1.2 추론 기반 기법 개요\n",
    "- 추론 기반 기법에서는 당연히 '추론'이 주된 작업이다.\n",
    "- 추론 이란 주변 단어(맥락)이 주어져 있을때 [ ? ]에 무슨 단어가 들어가는지를 추측하는 작업이다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/5f07895e-9aee-44eb-8c6e-4d14412cd162)\n",
    "\n",
    "- 추론 문제를 풀고 학습하는 것이 '추론 기반 기법'이 다루는 문제이다.\n",
    "- 이러한 추론 문제를 반복해서 풀면서 단어의 출현 패턴을 학습하는 것이다.\n",
    "- 모델 관점에서는 맥락이 주어지고 나타날 수 있는 단어들의 확률을 분석하는 것이다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/9eef6690-b8af-4ea4-b409-f8a53966ce43)\n",
    "\n",
    "- 이러한 메커니즘을 이용하여 말뭉치를 사용해 모델이 올바른 추측을 내놓도록 학습시킨다.\n",
    "- 그리고 그 학습의 결과로 단어의 분산 표현을 얻는 것이 추론 기반 기법의 최종 목표이다.\n",
    "- 추론 기반 기법도 통계 기반 기법 처럼 분포 가설에 기초(단어의 의미는 주변 단어에 의해 형성된다.)한다\n",
    "- 이처럼 둘다 '단어의 동시 발생 가능성'을 얼마나 잘 모델링하는가가 중요한 연구 주제이다.\n",
    "\n",
    "#### 3.1.3 신경망에서의 단어 처리\n",
    "- 신경망은 단어를 있는 그대로 처리 할 수 없으므로 단어를 '고정 길이의 벡터'로 전환 해야한다.\n",
    "- 이떄 one-hot 표현으로 변환 한다, one-hot 표현 이란 벡터의 원소중 1개만 1이고 나머지는 0인 벡터를 말한다.\n",
    "- 총 7개의 corpus ('you', 'say', 'goodbye', 'and', 'i', 'hello', '.')\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/e7ce77fe-eeb8-435c-9c16-cb0515662fa2)\n",
    "\n",
    "- corpus에 있는 모든 단어를 원핫 표현으로 나타낼 수 있다.\n",
    "- 이 벡터를 통해 단어를 신경망으로 처리할 수 있게 됬다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/01e127c8-dc51-4b03-afdf-9b0363f19d2d)\n",
    "\n",
    "- 완전 연결층을 통해 은닉층과 연결할 수 있다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/ccf0e071-6541-4435-b6a4-2d48becd580e)\n",
    "\n",
    "- 완전 연결 층은 행렬의 곱으로 표현 가능하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23411594 -0.30889829  0.97405564]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.array([1, 0, 0, 0, 0, 0, 0])\n",
    "W = np.random.randn(7, 3)\n",
    "h = np.matmul(c, W)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 완전연결계층의 계산은 행렬 곱으로 수행할 수 있고, 행렬 곱은 넘파이의 np.matmul()이 해결해 준다.\n",
    "\n",
    "![image](https://github.com/choibigo/Study/assets/38881179/549e1e7d-f350-4e55-b1fd-549ca6ce8815)\n",
    "\n",
    "- 위와 같이 c와 W의 행렬 곱은 결국 위 그림처럼 가중치의 행벡터 하나를 '뽑아낸' 것과 같다.(1에 해당하는 원소만 뽑는다.)\n",
    "- 원소 1개만 뽑아내는 작업을 행렬곱으로 계산하는 건 비효율 적이기 때문에 4.1장 word2vec 개선에서 개선할 예정이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
