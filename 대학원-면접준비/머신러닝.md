
# AI Deep Dive 정리

## MLE
- Maximum Likelihood Estimation이다.
- 어떠한 Measurement가 있을때 Measurement가 나오기 위한 조건들이 여러가지가 있다. 이때 여러 조건들에 대해 나올 확률 분포를 Likelihood라고 하고
- Measurement가 나올 가장 큰 확률의 조건을 찾는 것이 MLE이다.

## MAP
- Maximum A Posterior이다.
- MLE에서 조건에 대한 분포 즉 사전 확률을 알때 사용할 수 있는 방법이다.

## 인공지능이란
- 인간의 지능을 모델링 한 알고리즘 이다.

## Machine Learning이란
- 데이터를 기반으로 스스로 규칙을 만들어 모델링하는것

## Deep Learning
- Neural Network를 이용하여 모델링 한것

## 지도 학습
- 학습 시 데이터에 정답을 아는 경우 사용한다.
- 예를들어 Classification을 수행할 떄 데이터에 대한 정답 Label을 알려준다.

## 비지도 학습
- 학습 시 데이터에 정답을 모르는 경우 사용한다.
- 여러 데이터만 보고 Clustering을 수행할 때 사용된다.

## 자기지도 학습
- 지도 학습처럼 정답을 아는 데이터가 주어져 있지만 그 데이터 수가 적을때 주로 사용한다.
- 풀고자하는 문제를 바로 푸는것이 아닌 다른 문제를 먼저 풀어서 미리 학습한 후 실제 문제를 푸는 방법이다.
- 예를 들어 중간을 기준으로 8방향의 패치를 맞추는 문제나, Contrast Learning과 같이 같은 이미지에 나온 패치인지 확인하는 문제를 먼저 푼다.
- 이후 Output 부분 만 변경하여 원래 풀고자 하는 문제로 Transfer Learning을 수행한다.

## 강화학습
- 모델이 성공시에는 상을 실패시에는 벌을 주면서 학습하는 방법이다.
- 그예로 Q-Learning이 있다.

## 선형회귀
- 입력과 출련간의 관계를 설명하는 모델을 구하는 것을 회귀 라고 한다.
- 이때 모델이 선형적이라면 이를 선형 회귀 라고 한다.
- 선형회귀의 Loss 함수로 MSE를 주로 사용한다.

## Gradient Descent
- 파라미터(Weight, Bais 등 ..)가 Loss에 미치는 영향을 알기 위해 Loss 함수를 각 파라미터로 편미분한다.
- 이것의 집합을 Gradient라고 하는데 이때 Gradient는 함수의 가장 가파른 방향을 가르킨다.
- 그렇기 때문에 반대 쪽으로 이동하여 함수의 최소값을 찾을 수 있다.
- 원래 Weight 값에 Learning Rate와 Gradient를 곱한 값을 빼주어 Weight를 갱신한다.

## 가중치 초기화
- Xavior 방법이 있으며 주로 Sigmoid가 활성화 함수 일때 사용한다.
- He 방법은 활성화 함수가 ReLU일때 사용한다.
- 0을 기준으로 Random한 값을 갖도록 한다.

## Weight가 0이라면
- Gradient에가 0이기 때문에 학습이 일어나지 않는다.

## Weight가 상수라면
- Weight가 여러개인 의미가 없다.

## GD VS SGD
- GD는 한번 구할때 모든 데이터를 고려하기 때문에 시간이 오래걸린다.
- 또한 모든 데이터를 고려 하기 떄문에 Local Minimum에 빠졌을때 나오기 어렵다.
- 이러한 문제점을 해결하기 위해 SGD를 사용한데 Data중 Random하게 1개만 뽑아서 Gradient를 구하고 Weight를 갱신한다.
- 그렇기 때문에 Local Minimum에 빠져도 빠져 나오기 쉽다.

## Momentum
- 이전의 Weight를 업데이트한 방향과 현재 업데이트할 방향이 같다면 더크게 움직이는 방법이다.
- 이를 통해 관성을 갖게 하여 더 빠르게 Weight를 갱신할 수 있다.

## RMS Prop
- Weight를 업데이트 할때 너무 큰 쪽은 조금 작게, 너무 작은 쪽은 좀더 크게 변화시키는 방법이다.
- 이를 통해 평균화 하여 공평한 탐색을 할 수 있게 한다.

## Adam 
- Momentum과 RMS Prop 방식을 합친 방식이다.

## Dataset
- Training Dataset : 모델을 학습 할때 사용하는 데이터 셋이다.
- Test Dataset : 모델을 평가할 때 사용하는 데이터 셋이며, Training할때는 사용하면 안된다.
- Validation Dataset : 모델을 학습할때 중간중간 학습 상태를 보기위해 사용하는 데이터 셋이다. 또한 Hyperparameter를 결정할 때 사용한다
- Training Dataset을 통해 Hyperparameter를 결정하면 안된다.

## K-fold Cross Validation
- Dataset 자체가 너무 적어 Training Dataset과 Validation Dataset으로 나눌수 없는 경우 사용한다.
- 총 크기를 1/k로 쪼갠 만큼을 validation dataset으로 하여 학습 하는데 이때 겹치지 않게 k개의 데이터셋을 만들어 모델을 학습한다.
- 이후 k의 모델이 나온다 이를 바탕으로 Hyperparameter를 결정하여 모든 데이터셋을 Train set으로 사용하여 재학습을한다.

## 활성화 함수의 필요성
- 선형 함수를 활성화 함수를 사용하면 Layer를 여러층 쌓아서 표현력을 다양하게 하는 장점을 얻을 수 없다.
- 그렇기 떄문에 비선형 함수를 사용하여 Layer를 여러층 쌓아서 표현력을 다양하게 하여 데이터에 대한 복잡한 모델을 얻을 수 있다.
- 비선형 활성화 함수로 계단함수, Sigmoid, ReLU 등이 있다.

## Backpropagation
- Loss에 대해 각 파리미터가 영향을 미치는 정도를 알기위해 Loss에 대해 각 파라미터를 편미분한다.
- 이때 Chain Rule을 이용해 각 파라미터는 앞단에서 계산되어서 오는 값을 통해 Gradient를 알 수 있다.
- 이렇게 Loss 즉 오차를 뒤로 전달하는 것을 Backpropagation이라고 한다.

## Sigmoid
- 1 / 1 + e^-x 이다.
- 모든 구간에서 미분 가능하다.
- 미분 최대값이 1/4이기 때문에 Gradient Vanishing이 일어날 수 있다.

## Logistic Regression
- 데이터가 어떤 클래스에 해당하는지 Binary Classification에 사용된다.
- Logit을 Regression 하는 것이다.
- 이때 Logit은 각 클래스에 속할 확률을 Odd로 만들고 Log를 취한 것이다.
- 이 Logit을 Regression 하여 Class에 속할 확률을 얻을 수 있다.
- 이후 활성화 함수로 Sigmoid를 사용한다.

## MSE VS BCE
- Classification을 할때는 BCE를 사용한다.
- Classification의 Output은 0~1사이 확률 값이다. 이때 MSE그래프보다 BCE그래프가 더욱 Error에 민감하게 반응한다.
- 또한 MSE은 이전 입력에 제곱을 하게 되는데 이는 이전 입력의 Convexity를 더욱 Non-convex하게 만들게 된다. 따라서 학습을 더 어렵게 한다.

## Softmax
- 다중 분류를 하기 위해 사용한다.
- e^x1 / (e^x1 + e^x2 + ... e^xn)
- 절대값을 쓰지 않는 이유 : -1와 1을 구분하지 못한다, 미분 불가능한점이 생긴다, 분모가 0에 가까워 지면 계산이 불안정해 질 수 있다.

# 예상질문
## 인공지능
- 인간이 가지는 인식, 판단 등의 지적 능력을 모델링하여 구현한것

## 머신러닝이란?
- 데이터를 분석하여 데이터를 통해 학습된 내용을 바탕으로 결정을 내리는 알고리즘

## 딥러닝
-  머신 러닝의 한 종류중 하나이다, 뉴럴 네트워크를 통해 학습하는 방법으로 머신러닝의 한 종류이다.

## Linear Regression
- 회귀란 입력 데이터와 출력 데이터를 가장 잘 설명하는 관계 즉, 함수를 찾는 알고리즘 이다.
- 선형 회귀는 데이터간의 관계를 선형적으로 나타내는 알고리즘 이다.

## Loss Function
- 학습중에 학습한 결과와 실제 값과 차이를 보기 위한 방법이다.
- 모든 입력 데이터에 대해 오차를 계산하는 함수를 Cost Function 이라 한다.
- 모든 데이터의 오차, Loss Function의 평균이 된다.

## Sigmoid보다 ReLU를 많이 쓰는데 그이유는?
- Sigmoid는 함수의 특성상 파라미터 값이 0이나 1에 수렴하게 된다.
- 모델의 깊이가 깊어질 수록 파라미터가 특정값으로 수렴하면 기울기가 변화하지 않고 학습이 이루어 지지 않는 Gradient Vanishing 현상이 일어 난다.
- 그에 반해 ReLU는 출력값이 하나의 값으로 수렴하지 않아서 Backpropagation할 떄, 멀리있는 Layer까지 전달 된다.

## Non-Linearity라는 말의 의미와 필요성은
- 데이터에 맞는 분포를 잘 설명할 수 있는 모델을 좋은 모델이라고 합니다.
- 데이터의 복잡도가 높아지면 높은 차원이 필요하고 데이터의 분포는 단순히 선형 형태가 아닌 비선형 형태를 가지게 됩니다.
- 이러한 분포를 가지는 경우 1차식인 Linear한 형태로 데이터를 표현할 수 없기 때문에 Non-Linearity가 중요하게 됩니다.

## ReLU의 문제점
- ReLU의 문제점은 입력값이 0보다 작을 떄, 함수 미분값이 0이 되는 단점이 있다.
- 이를 해결하기 위해 Leaky ReLU를 사용한다.

## Bias는 왜?
- Bias는 모델이 데이터에 잘 Fitting 하기 위해 평행 이동하는 역할을 합니다.
- 데이터를 2차원으로 표현 했을때, 모든 데이터가 원점을 기준으로 분포해 있지는 않습니다.
- Bias를 이용하여 모델이 평면상에서 이동할 수 있도록 하고 데이터에 더 잘 fitting 될수 있도록 Bias또한 학습 합니다. 

## Sigmoid의 문제점
- 입력 값이 일정 범위를 넘어 가게 되면 0또는 1로 수렵하게 됩니다.
- 그렇게 되면 기울기 또한 0으로 수렴하여 학습 이제대로 되지 않습니다.

## Gradient Descent란?
- 기울기를 통해 함수의 해당 좌표에서 기울기를 통해 최대값이나 최소값을 찾아갈수 있습니다.
- Loss 함수를 최소로 만들기 위해 해당 기울기에 Learning Rate를 곱한 만큼 변화 하면서 최소하 할수 있습니다.

## 왜 Gradient Descent 에서는 기울기를 사용하는가?
- 기울기는 함수의 가장 가파른 방향을 가르킨다. 이때 가장 기울기의 반대 방향으로 가면 최소 방향을 알 수 있어 기울기를 사용한다.

## GD 중 Loss가 증가 하는 이유?
- Optimization 전략에 따라 Gradient가 양수인 방향으로도 Parameter가 증가 할 수 있다. 이 경우 Loss가 일시적으로 증가할 수 있다.
- Local Minimum을 탈출 하는 과정에서 일시 적으로 Loss가 증가할 수 있다. 

## 중학생에게 설명한다면
- 높은 산에서 앞이 보이지 않는 밤이 됬을 때 하산을 하기 위해서 밑으로 갈수 있는 방법은 현재 내위치의 방향을 토대로 가장 낮은 지점을 찾아갈 수 있다. 

## BackPropagation에 대해서 설명 한다면
- 실제 값과 예측값이 얼마나 차이가 나는지 구하고 오차에값을 다시 앞으로 전파하여 가중치를 업데이트 한다.
- Loss에 대한 가중치의 미분을 통해 가중치가 Loss에 미치는 영향을 알 수 있다. 
- 이때 연쇄법칙을 통해 앞단의 미분 값을 통해 해당 가중치의 미분값을 계산할 수 있다.

## Local Minima 문제에도 불구하고 딥러닝이 잘되는 이유는?
- Optimization 전략으로 Local Minma 문제를 어느정도 피할수 있기 때문이다.

## GD가 Local Minima문제를 피하는 방법은?
- Momentum 개념을 도입한 RMSprop나 Adam등의 Optimzation 전략을 사용한다.
- Batch 단위 만큼 보며 가중치를 업데이트 하여 Local Miminum을 탈출할 수 있다.

## 찾은 해가 GLobal Minimum인지 알 수 있는 방법은
- 수식적인 방법은 존재하지 않으나 Test Set을 이용하여 성능을 평가하여 학습된 모델이 Local Minimum에 가까운지 유추할 수있다.

## CNN에 대해서 아는대로 얘기하라
- Convolution 연산을 수행하는 Neural Network입니다.
- 하나의 이미지로부터 픽셀간의 연관성을 살린 여러가지 Feature Vector를 생성하는 과정의 반복 입니다.
- 이떄 Feature Vector를 뽑아내기 위해 Kernel을 사용하고 Kernel을 학습하는 방식으로 이미지를 더 잘 표현할 수 있는 kernel을 학습하여 최종 나타난 Feature Vector를 통해 이미지를 분류 및 다양항 Task에 이용합니다.

## CNN의 파라미터 개수를 계산해라
- 필터의 크기가 WxHxC 이고 개수가 N개라면 총 NxWxHxC개의 파라미터 개수가 필요하다.

## Training Set과 Test Set을 분리 하는 이유
- Training Set을 학습 하기 위해 사용하는 데이터 셋으로 학습의 일반화나 정확도를 판단하기 위해 학습에 관여하지 않는 Test Set과 분리 합니다.

## Validation Set이 있는 이유
- 매 학습 마다 Training Set의 학습 정도를 파악하고 학습시 Overfitting을 찾아내기 위해 사용합니다.

## Test Set이 오염 됬다는 의미는?
- Training set과 너무 유사하여 Test Data가 일반적인 상황을 재현하지 못하여 성능평가를 수행하지 못하는 의미 입니다.

## Regularization
- Cost Function값이 작아지는 방향으로 학습하다 보면 특정 가중치가 너무 커지는 현상이 나타난다. 이떄문에 Overfitting 문제가 발생할 수 있는데 특정 가중치가 너무 커지는 것을 방지 하기 위해 Cost Function 을 계산할 때 가중치의 절대값 만큼 더해주는 방법으로 특정 가중치값이 너무 커지는것을 방지하는 것이다.

## Bath Normalization 효과는?
- 은닉층의 입력값을 Normalization 함으로써 입력 분포를 조정할 수 있다. 
- 초기 데이터는 0근처 값이기 때문에 Learning Rate를 크게 잡아 학습을 빠르게 진행할 수 있다.
- 가중치의 초기 값에 대해 신경 쓰지 않아도 된다. 
- 특정 가중치가 커지는 것을 방지 할 수 있어 Overfitting 방지가 가능하다.

## Drop Out이란
- 의도적으로 특정 파라미터를 학습하지 않는 방법이다.
- 이 방법을 통해 특정 파라미터가 커지는 것을 방지한다.
- 이를 통해 모델의 일반화 성능을 높이고 Overfitting을 방지할 수 있다.

## BN을 적용해서 학습 이후 실제 사용시에 주의할 점은
- BN을 사용할시 학습시에는 입력 데이터에 대해 Normalization을 수행후 연산을 수행한다.
- 그러나 추론 상황에서는 학습시 사용한 배치 단위보다 작을 수 있기 때문에 학습시 사용했던 입력 파라미터의 평균과 분산을 저장해 두고 그 것들의 평균을 통해 normalization을 수행 해야 한다.

## GAN에서 BN이란
- 일반적으로 GAN에서는 BN을 사용하지 않는다. Discriminator가 조작되지 않는 Generator의 결과물을 정확한 값으로 판단하기 위해서 이다.

## SGD
- SGD는 파라미터의 기울기와 Learning Rate를 곱한 것을 빼 Loss Functino이 작아지는 쪽으로 다음 파라미터를 결정한다.

## Momentum
- Weight 변화가 관성을 갖게한 것으로, 이전에 변화한 방향과 현재 변화하는 방향이 같다면 더크게, 반대 방향이라면 더 적게 weight를 조정하는 방법이다.

## RMSprop
- RMSprop는 크게 변화해야하는 방향은 조금 가고 적게 변화하는 방향에 대해서 크게 변하게 Weight를 조정하는 방법이다.

## Adam
- Momentum 과 RMSprop 방식이 적용된 방법입니다.

## Stochastic Gradient Descent
- Train Dataset을 전체를 이용하는 것이 아닌 mini-batch를 통해 파라미터를 업데이트 한는 것을 의미한다.
- 이를 통해 Local Mimum을 탈출 할 수 있는 기회가 된다.

## 평가
![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99DC064C5BE056CE10)
- True Positive : 실제 True인 정답을 True라고 예측(정답)
- False Positive : 실제 False인 정답을 True라고 예측 (오답)
- False Negative : 실제 True인 정답을 False라고 예측 (오답)
- True Negative : 실제 False인 정답을 False라고 예측 (정답)

## Accuracy
- 전체 정답률
- class의 밸런스가 맞지 않을때는 accuracy 
- Imbalance class에서는 Accuracy 기준으로 판단하면 오류가 있기 때문에 Precision과 Recall로 판단 해야한다.

## Precision
- True라고 분류된 것들중 실제 True로 값들의 비율
- 내가 예측한것 들 중이 진짜 인 것들
- TP / (TP + FP)
- FP를 낮추는 데 초점

## Recall
- 실제 True 인 것중이 True라고 예측된 값들의 비율
- 실제 진짜 중에 내가 얼마나 검출 했나?
- TP / (TP + FN)
- 진짜 결함을 정상으로 판단하게 되어 영향일 더 크게 주는 경우

## F1 Score
- Data가 Imbalanced 할때, 정확도가 아닌 F1 Score를 사용한다.
- F1-Score = (2*Precision*Recall) / (Precision + Recall)
- Recall과 Precision의 조화평균이다.

<br>

![image](https://t1.daumcdn.net/cfile/tistory/99632A415E8BE0F53F)

## Pixel Accuracy
- Pixel 별로 정확도를 검출
- 배경이 많은 이미지에서 전부 0으로 검출했을경우 accuracy가 높아지는 문제점이 있다.

## mIou
- (GT 영역 n 예측 영역) / (GT 영역 U 예측 영역) 

## GAP
- Fully Connected 를 통해서 Featurer를 벡터로 만들면 위치적 정보가 손실된다.
- 채널별로 Average Pooling을 통해 Feature를 뽑으면 위치적 정보를 포함하는 Feature를 뽑을 수 있다.
- 이를 통해 Fully Connected 계층을 없앨수 있고 Fully Connected 계층보다 파라미터를 줄일 수 있다.(GAP 은 파라미터가 없다.)

## CAM
- N개의 클래스를 예측 할때 GAP를 하기전 Feature Map의 채널은 N개 이다.
- GAP에서 나온 결과를 각 Class 별로 예측할때 n개의 가중치가 생셩 된다.
- 이 가중치와 GAP 전 Convolution 계층을 Weighted Sum 을해 해당 클래스의 Heat Map을 나타낼 수 있고 중요도를 표시할 수 있다.

## SVM
- 두 클래스로 부터 가장 멀리 떨어져 있는 결정 경계를 찾는 분류기이다.
- 클래스를 분류하는 것을 목표로 한다.

## SVM에서 Margin
- 데이터 셋을 분리하는 경계선과 가장 가까운 데이터간 거리이다. 
- Margin은 집단간에 모두 같다.
- 데이터셋을 분리시킬 때 집단간 간격이 가능한한 가장 넓어야 하기 때문이다.
- 어떤 데이터가 경계에서 멀리 있을때 확선이 강해 지기 때문이다.
- 따라서 경계선을 각 집단을 가장 멀리 분리할 수 있도록 만든다.

## 차원의 저주
- 차원이 증가하면서 학습데이터 수가 차원 수보다 적어져서 성능이 저하되는 현상을 말한다.
- 차원이 증가할 수록 변수가 증가하고 개별 차원 내에서 학습할 데이터 수가 적어 진다. 따라서 학습이 잘 되지 않는다.

## RNN
- 입력과 출력을 시퀀스 단위로 처리하는 시퀀스 모델이다.
- 시퀀스란 문장 같은 단어가 나열된 것을 뜻한다.
- 기존 NN과 다르가 활성화 함수의 결과가 출력층에만 내보내는 것이 아니라 다른 은닉승 노드에 입력으로 보내는 특징이 있다.
- 연속적인 데이터를 다룰때 주로 사용하며, 번역기, 음성인식, 시계열 데이터 예측 등에서 사용한다.
- RNN 은 일찍 들어온 데이터는 잊혀 지는 현상이 있고 늦게 들어온 데이터를 더 중요하게 보는 현상이 있다.

## LSTM
- RNN의 문제를 해결하기 위한 모델이다.
- 직전 데이터 뿐만 아니라, 좀 더 거시적으로 과거 데이터를 고려하여 미래 데이터를 예측하기 위해 나온 모델이다.

## Attenion
- 주목해야 하는 데이터를 학습한다.
- RNN과 Attension을 결합하여 트랜스포머로 발전 했다.

## MLE
- Maximum Likelihood는 가장큰 Likelihood를 찾는 것으로 
- likelihood 는 Measure가 조건 x로 부터 나왔을 가능도를 의미한다.
- likelihood 함수를 미분해서 최대값을 알수 있으며 이때 계산의 편의를 위해 Log를 취해서 구한다

## MAP
- MLE에서 Prior Distribution을 알때 사용하는 방법이다.
- 정확한 Prior Distribution은 안다면 MLE보다 정확한 값을 얻을 수 있다.