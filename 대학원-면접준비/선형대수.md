
# 정리

## Transpose
- 대각 원소를 제외한 나머지 원소들의 자리를 바꾸는 것
- i행 j열 원소가 j행 i열 원소와 자리를 바꾼다.

## 내적
- 내적을 통해 하나의 벡터에 다른 하나의 벡터를 정사영 내린 크기를 와 내린 벡터의 크기의 곱을 알 수 있다.
- 또한 벡터들 간의 닮은 정도를 알 수 있다.
- a^t * b 와 같이 표현할 수 있다.
- 서로 직교 하다면 두 벡터의 내적은 0 이된다.

## Norm
- 벡터의 크기를 알 수 있다. 루트(a^t * a) 처럼 자기 자신을 내적하여 2-norm을 알 수 있다.
- 같은 벡터 크기라면 1-norm이 가장크고 infinite norm 이 가장 작다.
- 각 원소의 절대값의 n승을 모두 더한뒤 1/n제곱 하면 n-norm 이다.

## 행렬의 곱의 관점
1. 내적
2. Column Space
3. Rank-1 Matrix의 합

## Span
- 여러 벡터들의 Linear Combination으로 표현할 수 있는 Vector 공간이다.

## Column Space
- 행렬의 Column 벡터들을 Span하여 나타낼 수 있는 벡터 공간이다.
- Span 이란 벡터들을 Linear Combination 하여 나타낼 수 있는 공간이다.

## 선형 독립
- 하나의 벡터로 다른 벡터를 표현할 수 없다면 두벡터는 선형독립이라 한다.
- 하나의 벡터로 다른 벡터를 표현할 수 있다면 두벡터는 종속이라 한다.

## 기저
- 특정 Space를 나타낼 수 있는 선형독립인 벡터들 이다.
- 만약 3차원 공간이라면 3개의 기저로 표현가능하고
- n차원 공간이라면 n개의 기저로 표현가능하다.

## 역행렬
- 왼쪽에 원래 행렬 오른쪽에 항등 행렬을 놓고 증강 행렬을 만든다. 이후 가우스-조던 소거법을 통해 왼쪽 행렬을 항등행렬로 만들었을때 오른쪽에 오는 행렬이 역행렬이다.
- 어떠한 행렬에 곱했을때 항등행렬이 나오게 하는 행렬을 역행렬이라 한다.

## Orthogonal Matrix
- 모든 Column 벡터 들이 Orthnormal한 벡터들이다.
- Orthnormal이란 서로 직교 하면서 크기가 1인 벡터들이다.

## Rank
- 행렬의 Column Space가 갖는 차원우 수이다.
- 행렬의 Column Vector들 중 Independent한 Vector의 수이다.

## Null Space란
- Ax = 영벡터 를 만족하는 x벡터의 이다.
- 이때 만족하는 x벡터들은 무한하지만 무한한 집합을 대표하는 하나의 vector를 대표값으로 삼는다.
- 행렬 A의 Column 수는 A행렬의 Rank 수 더하기 영벡터의 개수로 표현된다.

## Ax=b 해의 개수

#### A가 Full Rank
- A의 역행렬이 존재 하므로 해의 개수는 1개이다.

#### A가 Rank-deficient
- b가 A위에 있으면 개수가 무한하나, A위에 없다면 해는 없다.

#### Full-column Rank
- 해가 1개 있거나 없다.
- 해가 A위에 있다면 1개 있다. (Full-column Rank 이므로 Null Space 가 0차원)
- 해가 A위에 없다면 해가 없다.

#### Full-row Rank
- 1개의 해는 항상 존재하고 null space가 무한개 있으므로 최종적으로 해는 무한하다

## 가우스-조던 소거법
- 연립 방정식을 확장 행렬로 만든다.
- 행의 위치를 바꾸거나, 행에 스칼라배, 또는 행끼리 덧샘을 통해 왼쪽 부분을 항등 행렬로 만든다.
- 최종적으로 오른쪽에 남아있는 값들이 연립방정식의 해가 된다.

## Determinate
- 행렬을 입력으로 하여 하나의 스칼라 값이 나오는 어떠한 함수이다.
- Determinate의 값에 따라 행렬의 역행렬 유무를 결정할 수 있다.

## Trace
- 행렬의 대각 원소들 끼리 합이다.

## 최소차승법
- 어떠한 space로 표현하지 못하는 vector을 그 space로 최대한 표현하기 위한 방법이다.
- 이때 최소의 오류로 표현할 수 있는 방법은 그 Space에 벡터를 정사영 내린 벡터로 표현하는 것이다.

##

# 예상 질문

## Linearly Independent란?
- 벡터끼리 선형 독립을 의미 하며 
- 어떠한 벡터들의 스칼라배와 합으로 영벡터를 표현할때 그 스칼라 배가 0밖에 없다면 그 벡터들은 Lineary Independent라고 말할 수 있다.
- 벡터 집합이 있을때 어떠한 원소도 나머지의 선형 결합으로 나타낼 수 없을때 선형 독립 이라 한다.

## Vector Space란?
- Vector들이 Span하여 나타낼 수 있는 공간을 Vector Space라고 한다.

## Subspace
- 전체 기저 벡터중 특정 개수만 사용 하여 만드는 공간이다.
- 3개의 기저 벡터가 있을 떄 2개를 사용하여 만든 subspace는 면이 된다.

## Basis & Dimension이란?
- 특정 Space를 선형 결합으로 나타낼 수 있는 벡터들의 집합이다.
- Basis의 개수는 그 Space의 Dimension 수 이다.

## Column Space, Row Space
- 행렬의 열 벡터들로 Span 하여 나타낼 수 있는 공간을 Column Space라고 한다.
- 행렬의 행 벡터들로 Span 하여 나타낼 수 있는 공간을 Row Space라고 한다.
- Column Space의 차원과 Row Space의 차원은 항상 같다.

## Span 이란?
- Vector들의 선형 결합으로 나타 낼수 있는 Vector Space이다.
- 독립 벡터의 개수에 따라 나타낼 수 있는 공간의 차원이 결정 된다.

## Null Space란?
- Ax = 0 을 만족하는 x의 집합니다.
- 이때 벡터 x는 무한개가 나올 수 있으나 통상 Column Space의 차원에 따라 선형 독립인 벡터 x를 고른다. 
- Column Space의 차원 수와 Null Space의 차원 수를 더하면 행렬 A의 열의 개수가 된다.

## Symmetric Matrix란?
- 전치 했을때 생기는 행렬과 자기 자신이 같은 행렬

## positive definite란?
- Eigen Value가 모두 양수인 Symmetric Matrix이다.
![image](https://user-images.githubusercontent.com/38881179/232941483-ebe131e9-6c1c-44e4-88ef-8706ce5d8cc6.png)

- x와 Ax를 내적
- x와 Ax간의 내적 결과가 양수 여야 한다. => 원래 있는 기존 x벡터와 선형 변환 이후에 벡터가 이루는 각이 90도 이하인 행렬 A
- [참고 링크](https://angeloyeo.github.io/2021/12/20/positive_definite.html)
- 최적화 분야에서 중요하게 사용된다.

## Rank란?
- 행렬의 Column Space의 차원수를 의미 한다.
- Column Space란 행렬의 Column Vector들이 Span하는 공간이다.

## Determinat가 의미하는 바는 무엇 인가?
- 정사각 행렬을 하나의 스칼라 값으로 만드는 함수이다.
- 행렬식의 값에 따라 역행렬 존재 여부를 판단 할 수 있으며 행렬식이 0 일때 역행렬이 없다고 판단한다.
- 기하학 적으로 선형 변환했을때 (0,1)과 (1,0)의 넓이가 변환 했을때 넓이는 ad-bc(행렬식)가 된다.

## 선형 변환이란
- Ax와 같이 벡터 x에 행렬 A를 곱하는 행위를 선형 변환이라 할 수 있다.
- 선형 변환이란 변환 전 후의 원점이 이동하지 않고 격자간의 간격은 동일하다.
- [2,-3
   1,1] 행렬이 있을때 (1,0)의 x축은 (2,1)이 되고 (0,1)의 y축은 (-3, 1)이 된다.
- 선형 변환에 매칭 되는 행렬이 존재 한다.

## Eigen Value란? Eigen Vector란?
- 벡터에 행렬을 곱하는 행위를 벡터를 선형 변환 했다고 말할수 있다.
- 선형 변환 이후에 방향이 유지되고 크기만 변화하는 벡터를 Eigen Vector라고 하며 그때 변하는 크기를 Eigen Value라고 한다.

## Eigen Vector는 왜 중요한가?
- 고유값 고유벡터를 이용해 Eigen Decomposition이나 더 나아가 SVD를 하여 데이터 압축이나 차원 축소를 할 수 있습니다. 또한 Pseudo Inverse를 구할 수 있어 중요하다고 생각합니다.

## SVD란? 중요한 이유는?
- 기존 Eigen Decomposition은 정사각 행렬과 Symmetrix 행렬인 경우에만 사용이 가능 했습니다.
- 그러나 SVD를 통해 Square Matrix에 대해서도 Decomposition이 가능 해 졌고 모든 행렬에 대해 데이터 압축이나 차원 축소를 수행할 수 있게 됐습니다.

## Jacobian Matrix란 무엇인가?
- 비선형 변환이라도 비선형 변환을 좁은 영역을 본다면 선형 변환으로 근사할 수 있다. 
- 비선형 변환의 국소적 영역을 선형 변환으로 근사하는 방법

- 기존 좌표계에서 변환 좌표계로 변화하는 행렬을 구한다.
- 그 행렬의 원소는 기존 좌표계에 대해 변환 좌표계로 편미분한 값이 된다.

#### 비선형 변환
- 원점이 이동 하며 변환 후에 격자 간의 간격이 동일하지 않는 변환


## 역행렬이란?
- 어떠한 행렬을 곱하여 항등 행렬이 된다면 그 행렬의 역행렬이라 한다.
- 왼쪽에 기존 행렬을 두고 오른쪽에 항등 행렬을 둔 증강 행렬을 만든다.
- 이후에 가우스-조던 소거법을 통해 왼쪽 행렬을 항등 행렬로 만든 이후 오른쪽에 있는 행렬이 역행렬이다.

## PCA(Principle Component Analysis)란 무엇인가?
- 데이터 차원 축소를 하는 방법이다.
- 데이터들의 분포를 가장 잘 나타내는 벡터를 찾기 위해 데이터와 그 벡터의 차이가 가장 적은 벡터를 찾는다. 이때 데이터의 공분산 행렬의 Eigen Value가 가장큰 Eigen Vector가 주성분 이며, 그 벡터에 데이터를 정사영 내려 데이터를 축소할 수 있다.