## Linearly Independent란?
- 벡터끼리 선형 독립을 의미 하며 
- 어떠한 벡터들의 스칼라배와 합으로 영벡터를 표현할때 그 스칼라 배가 0밖에 없다면 그 벡터들은 Lineary Independent라고 말할 수 있다.
- 벡터 집합이 있을때 어떠한 원소도 나머지의 선형 결합으로 나타낼 수 없을때 선형 독립 이라 한다.

## Vector Space란?
- Vector들이 Span하여 나타낼 수 있는 공간을 Vector Space라고 한다.

## Subspace
- 전체 기저 벡터중 특정 개수만 사용 하여 만드는 공간이다.
- 3개의 기저 벡터가 있을 떄 2개를 사용하여 만든 subspace는 면이 된다.

## Basis & Dimension이란?
- 특정 Space를 선형 결합으로 나타낼 수 있는 벡터들의 집합이다.
- Basis의 개수는 그 Space의 Dimension 수 이다.

## Column Space, Row Space
- 행렬의 열 벡터들로 Span 하여 나타낼 수 있는 공간을 Column Space라고 한다.
- 행렬의 행 벡터들로 Span 하여 나타낼 수 있는 공간을 Row Space라고 한다.
- Column Space의 차원과 Row Space의 차원은 항상 같다.

## Span 이란?
- Vector들의 선형 결합으로 나타 낼수 있는 Vector Space이다.
- 독립 벡터의 개수에 따라 나타낼 수 있는 공간의 차원이 결정 된다.

## Null Space란?
- Ax = 0 을 만족하는 x의 집합니다.
- 이때 벡터 x는 무한개가 나올 수 있으나 통상 Column Space의 차원에 따라 선형 독립인 벡터 x를 고른다. 
- Column Space의 차원 수와 Null Space의 차원 수를 더하면 행렬 A의 열의 개수가 된다.

## Symmetric Matrix란?
- 전치 했을때 생기는 행렬과 자기 자신이 같은 행렬

## positive definite란?
- Eigen Value가 모두 양수인 Symmetric Matrix이다.
![image](https://user-images.githubusercontent.com/38881179/232941483-ebe131e9-6c1c-44e4-88ef-8706ce5d8cc6.png)

- x와 Ax를 내적
- x와 Ax간의 내적 결과가 양수 여야 한다. => 원래 있는 기존 x벡터와 선형 변환 이후에 벡터가 이루는 각이 90도 이하인 행렬 A
- [참고 링크](https://angeloyeo.github.io/2021/12/20/positive_definite.html)
- 최적화 분야에서 중요하게 사용된다.

## Rank란?
- 행렬의 Column Space의 차원수를 의미 한다.
- Column Space란 행렬의 Column Vector들이 Span하는 공간이다.

## Determinat가 의미하는 바는 무엇 인가?
- 정사각 행렬을 하나의 스칼라 값으로 만드는 함수이다.
- 행렬식의 값에 따라 역행렬 존재 여부를 판단 할 수 있으며 행렬식이 0 일때 역행렬이 없다고 판단한다.
- 기하학 적으로 선형 변환했을때 (0,1)과 (1,0)의 넓이가 변환 했을때 넓이는 ad-bc(행렬식)가 된다.

## 선형 변환이란
- Ax와 같이 벡터 x에 행렬 A를 곱하는 행위를 선형 변환이라 할 수 있다.
- 선형 변환이란 변환 전 후의 원점이 이동하지 않고 격자간의 간격은 동일하다.
- [2,-3
   1,1] 행렬이 있을때 (1,0)의 x축은 (2,1)이 되고 (0,1)의 y축은 (-3, 1)이 된다.
- 선형 변환에 매칭 되는 행렬이 존재 한다.

## Eigen Value란? Eigen Vector란?
- 벡터에 행렬을 곱하는 행위를 벡터를 선형 변환 했다고 말할수 있다.
- 선형 변환 이후에 방향이 유지되고 크기만 변화하는 벡터를 Eigen Vector라고 하며 그때 변하는 크기를 Eigen Value라고 한다.

## Eigen Vector는 왜 중요한가?
- 고유값 고유벡터를 이용해 Eigen Decomposition이나 더 나아가 SVD를 하여 데이터 압축이나 차원 축소를 할 수 있습니다. 또한 Pseudo Inverse를 구할 수 있어 중요하다고 생각합니다.

## SVD란? 중요한 이유는?
- 기존 Eigen Decomposition은 정사각 행렬과 Symmetrix 행렬인 경우에만 사용이 가능 했습니다.
- 그러나 SVD를 통해 Square Matrix에 대해서도 Decomposition이 가능 해 졌고 모든 행렬에 대해 데이터 압축이나 차원 축소를 수행할 수 있게 됐습니다.

## Jacobian Matrix란 무엇인가?
- 비선형 변환이라도 비선형 변환을 좁은 영역을 본다면 선형 변환으로 근사할 수 있다. 
- 비선형 변환의 국소적 영역을 선형 변환으로 근사하는 방법

- 기존 좌표계에서 변환 좌표계로 변화하는 행렬을 구한다.
- 그 행렬의 원소는 기존 좌표계에 대해 변환 좌표계로 편미분한 값이 된다.

#### 비선형 변환
- 원점이 이동 하며 변환 후에 격자 간의 간격이 동일하지 않는 변환


## 역행렬이란?
- 어떠한 행렬을 곱하여 항등 행렬이 된다면 그 행렬의 역행렬이라 한다.
- 왼쪽에 기존 행렬을 두고 오른쪽에 항등 행렬을 둔 증강 행렬을 만든다.
- 이후에 가우스-조던 소거법을 통해 왼쪽 행렬을 항등 행렬로 만든 이후 오른쪽에 있는 행렬이 역행렬이다.

## PCA(Principle Component Analysis)란 무엇인가?
- 데이터 차원 축소를 하는 방법이다.
- 데이터들의 분포를 가장 잘 나타내는 벡터를 찾기 위해 데이터와 그 벡터의 차이가 가장 적은 벡터를 찾는다. 이때 데이터의 공분산 행렬의 Eigen Value가 가장큰 Eigen Vector가 주성분 이며, 그 벡터에 데이터를 정사영 내려 데이터를 축소할 수 있다.