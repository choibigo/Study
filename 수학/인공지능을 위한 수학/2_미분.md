# 2-1 극한

#### 극한 
- 수열이나 함숫값이 어떤 특정값에 한없이 가까워지는 것을 의미한다.

![image](https://velog.velcdn.com/images%2Fwlsn404%2Fpost%2F438e7d85-2a0d-4f28-9cc9-9febcddf2b72%2Fimage.png)

- x=1일떄 분모가 0이 되기 때문에 y를 정의 할수 없다.
- x≠1인 경우 f(x)를 정의할 수 있다.
- x를 1.1 1.01 0.999 ... 와 같이 최대한 1에 가깝게 만들어 주면 f(x)를 구할 수 있다.
- 이렇게 x의 값을 어떠낳 a에 최대한 가깝게 만들 때, 함수 f(x)의 값도 어떤 값 a에 최대한 가까워지는 모양을 수렴 이라고 한다.

<br>
![image](https://velog.velcdn.com/images%2Fwlsn404%2Fpost%2F23860c42-41bf-4f14-aa38-57d7fea593c7%2Fimage.png)
- a는 함수 f(x)에서 x->a일때의 극한값 이라고 한다.

# 2-2 미분의 기초

#### 미분
- 아주 짧은 순간의 함수에 대한 기울기를 구하는 것

- 자동차의 이동 거리를 x, 자동차의 이동 시간을 t, 시간이 t일때 자동차의 위치를 x(t)라고 가정할때, 순간 속도는 다음과 같다.
![image](https://velog.velcdn.com/images%2Fwlsn404%2Fpost%2F780f89da-7774-46e9-a2eb-b65471cd62a9%2Fimage.png)
- 시간의 변화량을 최대한 0에 가깝게 만들때 순간 속도가 얼마인지, 얼마나 변하는지를 알기 위한 식이다.

![image](https://velog.velcdn.com/images%2Fwlsn404%2Fpost%2F68832368-c24a-482c-82fc-1649ffb850f0%2Fimage.png)
- 이동 시간의 변화량을 0에 가깝게 극한으로 만들 때의 이동 거리의 변화량이 미분인 것이다.

![image](https://velog.velcdn.com/images%2Fwlsn404%2Fpost%2F337b88dc-6d10-4b08-884e-c02986174d9f%2Fimage.png)
- x가 dx만큼 아주 조금 변화할 때 함수 f(x)가 얼마나 변화(df(x))하는지를 아주 짧은 순간의 변화율로 표현하고 있다.

![image](https://velog.velcdn.com/images%2Fwlsn404%2Fpost%2F74a44525-ac58-422a-baed-d07451a870e2%2Fimage.png)
- 직선의 기울기 α는 두 점사이에서 평균적으로 변화한 정도 이다. 
- 이때 f(x) 위의 점(a, f(a))에서 '순간적으로 변화한 정도'인 기울기를 구하기 위해서 b의 값을 a에 가깝게 이동한다.
- 이를 통해 a점에서 기울기를 구할 수 있고 이를 '미분'이라고 한다. 
- 이떄 직선을 a점에 대한 접선 이라고 하며 그 기울기 α는 미분계수 라고 한다.

#### 인공지능에서
- 인공지능 분야에서는 함수의 값이 어느 지점에서 최소가 되는지 알아내는 것이 중요하다.
- 손실함수는 정답과 예측값 사이의 오차를 표현하는 함수이다.
- 손실 함수를 미분하면 어떤 특정지점에서 어느 정도 기울기가 나오는지 알수 있고, 이러한 기울기의 절대값이 작아지는 방향으로 학습 하면 손실함수의 최소값을 구할 수 있다.

# 2-3 상미분과 편미분

#### 상미분
- 변수가 하나만 있는 함수의 미분

#### 전미분
- 변수가 2개 이상 있는 함수의 미분

![image](https://velog.velcdn.com/images%2Fwlsn404%2Fpost%2Fef1e42a0-e344-4da0-af0b-4097e1c05205%2Fimage.png)
- 이후 x변화량, y변화량 -> 0과 같이 극한 값을 구하면 함수 f(x, y)의 미분을 구할 수 있다.
- 이러한 미분을 전 미분 이라고 한다.
- 전미분을 하는 방식중 하나의 변수를 제외하고 나머지 변수를 고정하는 방식을 편미분 이라고 한다.

![image](https://velog.velcdn.com/images%2Fwlsn404%2Fpost%2Fe6e5adbf-bebb-4545-8924-68b4ca3c4015%2Fimage.png)
- 아래 첨자를 사용해서 간단하게 표현할 수 있다.

# 2-4 그래프 그리기 
- 그래프에서 볼록한 정점은 그 지점의 근방에서 가장 큰값을 가지개 되며 이를 극대값 이라고 하고, 그 점을 극대점이라고 한다.
- 아래로 볼록한 정점은 그 지점의 근방에서 가장 작은 값을 자기게 되고 극소값 이라고 하며 그 지점을 극소값이라고 한다.
- 극대값과 극소값을 가지는 지점을 그 지점의 앞뒤에서 미분계수의 값이 양에서 음으로, 또는 음에서 양으로 바뀌는 지점 이다.

# 2-5 함수의 최대값과 최소값
- 일반적으로 함수의 최대값, 최소값은 극점이나 구간의 양 끝단에서 나온다.
- 한번 미분한 값이 0 일때 극값을 구할 수 있다.

#### 인공지능에서
- 최소제곱법은 여러개의 오차들을 제곻바고, 그 제곱들의 합이 최소가 되는 관계식을 구하는 것으로, 선형회귀 알고리즘에서 사용하는 최적화 기법 이다.
- f(a, b)= (오차의 제곱들의 합) 이라고 가장할 떄, fa(a,b)'=0, fb(a,b)'=0 방정식을 풀면 오차의 제곱들의 합한 결과가 최소가 되는 관계식을 풀 수 있다.

# 2-6 초등함수와 합성 함수의 미분, 곱의 법칙

#### 초등함수
- 멱함수, 지수 함수, 로그함수, 삼각 함수 등

#### 변수가 1개인 y=f(x)미분
![image](https://velog.velcdn.com/images%2Fwlsn404%2Fpost%2Fec79c5ce-6b20-4c34-9dc3-5fbd3d8adb7b%2Fimage.png)
- 합성함수의 미분 또는 연쇄 법칙 이라고 한다.
- 임의식을 여러개 끼워 넣어서 계산 할 수 있다.

#### 변수가 여러개인 y=f(x)미분
![image](https://velog.velcdn.com/images%2Fwlsn404%2Fpost%2F94822ec2-d64c-42ba-94f1-fa3da034f7e1%2Fimage.png)
???

#### 인공지능에서
- 신경망에서는 학습한 결과로 도출된 답이 정답과 얼마나 차이가 있는 지 계산 후 가까워 지도록 가중치를 조정하는 것을 반복한다.
- 실제 정답과 학습 결과 사이의 오차 값을 가중치로 편미분 한 다음, 그 값을 가중치의 조정량으로 사용한다.
- 이렇게 오차로부터 가중치의 미분 값을 계산하는 과정에서 연쇄법칙을 이용하며 이를 오차 역천파법 이라고 한다.

# 2-7 특수 함수의 미분

#### Sigmoid
![](https://velog.velcdn.com/images%2Fwlsn404%2Fpost%2Faf1cbdfa-a08f-4edb-b5f2-574dcde12b4b%2Fimage.png)
![](https://velog.velcdn.com/images%2Fwlsn404%2Fpost%2Ff0d5a94a-f481-44bc-baa7-249968177edd%2Fimage.png)
- 시그모이드 함수를 미분 했을 때의 최대값은 0.25이다.
- sigmoid를 활성화 함수로 사용하면 오차 역전파 과정에서 0보다 작은 값이 계속 곱해져 낮은 layer로 갈 수록 미분값이 0이되어 오차가 잘 전달 되지 않는 Gradient Vanishing 현상이 일어날 가능성이 높아 진다.

#### ReLU
![](https://velog.velcdn.com/images%2Fwlsn404%2Fpost%2F3c34c2e8-1fef-4149-b4bc-c7c6460d7b51%2Fimage.png)
- Gradient Vanishing 현상을 방지하기 위해  ReLU 함수를 사용한다.
- ReLU는 입력이 0 이하인 경우 미분값이 0이 된다. 따라서 Sigmoid와 같은 문제가 나타날 수 있고, 이를 해결하기 위해 LeakyReLU가 사용된다.